{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2328b69",
   "metadata": {},
   "source": [
    "# KNN and A Standard Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ddb084",
   "metadata": {},
   "source": [
    "In this document we showcase how a standard feature manufacturing and selecting pipeline works, and investigate into cv loop of KNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c61d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import f_classif\n",
    "import itertools\n",
    "import sys\n",
    "import importlib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, make_scorer\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from threadpoolctl import threadpool_limits\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from proj_mod import training\n",
    "importlib.reload(training);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc042e",
   "metadata": {},
   "source": [
    "To ensure that there is no data leakage, the pipeline has to be fully automatic in evaluating the data, ranking importance, feature manufacturing, and finally, produce the model all ONLY on the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e260a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/raw.csv\")\n",
    "features=list(df.columns)[1:]\n",
    "target=[\"Y\"]\n",
    "feat=df[features]\n",
    "tar=df[target]\n",
    "x_t, x_v, y_t, y_v= train_test_split(feat,tar, test_size=0.2, random_state=0, stratify=tar[\"Y\"])\n",
    "n_splits=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e1a14",
   "metadata": {},
   "source": [
    "## Evaluating data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ebc44",
   "metadata": {},
   "source": [
    "With the input, we first analysis with Anova F test to rank importance of the raw features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a715c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>9.859455</td>\n",
       "      <td>0.002233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X5</td>\n",
       "      <td>4.947507</td>\n",
       "      <td>0.028423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.708349</td>\n",
       "      <td>0.057039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X3</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>0.271091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X4</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.612073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X2</td>\n",
       "      <td>0.160949</td>\n",
       "      <td>0.689158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  features   f score   p value\n",
       "0       X1  9.859455  0.002233\n",
       "4       X5  4.947507  0.028423\n",
       "5       X6  3.708349  0.057039\n",
       "2       X3  1.225000  0.271091\n",
       "3       X4  0.258824  0.612073\n",
       "1       X2  0.160949  0.689158"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_score, p_value = f_classif(X=x_t,y=y_t[\"Y\"])\n",
    "df_f=pd.DataFrame(({\"features\":x_t.columns, \n",
    "                    \"f score\": f_score, \n",
    "                    \"p value\": p_value}))\n",
    "df_f.sort_values(by=[\"f score\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25090a7b",
   "metadata": {},
   "source": [
    "## Feature manufacturing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056be76e",
   "metadata": {},
   "source": [
    "We describe how training.data_creator and training.data_selector behave: \n",
    "\n",
    "Keep in mind that input X of fit() DO NOT have to be input X of transform(). \n",
    "\n",
    "* data_creator: \n",
    "    - fit(): Using the X (features) and y (target) to learn f_arr_:=f_classif(X=X[features],y=y[\"Y\"])[0]. \n",
    "    - transform(): Create new features \"mean\", \"f_w_mean\", \"above_3\", \"above_4\", and \"above_5\" just like below for input X of transform. The only key point is that f_arr_ is learned in fit and applied here. \n",
    "\n",
    "* data_selector: \n",
    "    - fit(): Using X (features) and y (target) to learn feat_sel_ which consists of selected features based on anova f test f score and p value. \n",
    "    - transform(): Using the feat_sel_ to select only the wanted column of input X of transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3324cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t_copy=x_t.copy(deep=True)\n",
    "x_t_copy[\"mean\"]=x_t_copy[features].mean(axis=1)\n",
    "x_t_copy[\"F_w_mean\"]=x_t_copy[features].apply(lambda row_arr: (np.array(row_arr)* f_classif(X=x_t_copy[features],y=y_t[\"Y\"])[0]).mean(), axis=1)\n",
    "# x_t_copy[\"mi_w_mean\"]=x_t_copy[features].apply(lambda row_arr: (np.array(row_arr)*mutual_info_classif(X=df2[features],y=df2[\"Y\"],discrete_features=True)).sum()/len(features), axis=1)\n",
    "x_t_copy[\"above_3\"]=x_t_copy[features].apply(lambda row_arr: (row_arr >= 3).mean(), axis=1)\n",
    "x_t_copy[\"above_4\"]=x_t_copy[features].apply(lambda row_arr: (row_arr >= 4).mean(), axis=1)\n",
    "x_t_copy[\"above_5\"]=x_t_copy[features].apply(lambda row_arr: (row_arr >= 5).mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f04fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>10.289766</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>15.470365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>12.703944</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>14.629468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>10.289766</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>12.615471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>11.586720</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.716028</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>15.604569</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>13.773472</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    X1  X2  X3  X4  X5  X6      mean   F_w_mean   above_3   above_4   above_5\n",
       "28   3   3   1   3   3   4  2.833333  10.289766  0.833333  0.166667  0.000000\n",
       "46   5   3   3   4   4   5  4.000000  15.470365  1.000000  0.666667  0.333333\n",
       "67   4   1   1   3   4   4  2.833333  12.703944  0.666667  0.500000  0.000000\n",
       "3    5   4   3   3   3   5  3.833333  14.629468  1.000000  0.500000  0.333333\n",
       "27   3   3   1   3   3   4  2.833333  10.289766  0.833333  0.166667  0.000000\n",
       "..  ..  ..  ..  ..  ..  ..       ...        ...       ...       ...       ...\n",
       "57   4   4   4   4   3   4  3.833333  12.615471  1.000000  0.833333  0.000000\n",
       "39   4   4   3   4   2   4  3.500000  11.586720  0.833333  0.666667  0.000000\n",
       "94   3   1   3   2   1   2  2.000000   7.716028  0.333333  0.000000  0.000000\n",
       "53   5   2   4   3   4   5  3.833333  15.604569  0.833333  0.666667  0.333333\n",
       "31   4   1   3   4   4   5  3.500000  13.773472  0.833333  0.666667  0.166667\n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e5cfe6",
   "metadata": {},
   "source": [
    "## Ranking importance with consideration of colinearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695e4d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.302341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X2</th>\n",
       "      <td>0.003575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.202463</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.466674</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>-0.040493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3</th>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.202463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282279</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X4</th>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.282279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>0.051323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X5</th>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.219223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X6</th>\n",
       "      <td>0.376458</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.190947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.234216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F_w_mean</th>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.329373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_3</th>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.466674</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.056634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_4</th>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>0.206611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_5</th>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0.302341</td>\n",
       "      <td>-0.040493</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.051323</td>\n",
       "      <td>0.219223</td>\n",
       "      <td>0.190947</td>\n",
       "      <td>0.234216</td>\n",
       "      <td>0.329373</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>0.206611</td>\n",
       "      <td>0.235986</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                X1        X2        X3        X4        X5        X6  \\\n",
       "X1        1.000000  0.003575  0.197993  0.033369  0.337179  0.376458   \n",
       "X2        0.003575  1.000000  0.202463  0.054034  0.009242 -0.087204   \n",
       "X3        0.197993  0.202463  1.000000  0.282279  0.296808  0.186113   \n",
       "X4        0.033369  0.054034  0.282279  1.000000  0.209190  0.227200   \n",
       "X5        0.337179  0.009242  0.296808  0.209190  1.000000  0.283442   \n",
       "X6        0.376458 -0.087204  0.186113  0.227200  0.283442  1.000000   \n",
       "mean      0.528688  0.413327  0.664461  0.522414  0.661435  0.542631   \n",
       "F_w_mean  0.841993  0.019487  0.393921  0.211180  0.732961  0.607012   \n",
       "above_3   0.139387  0.466674  0.382278  0.379861  0.473777  0.287410   \n",
       "above_4   0.443484  0.276085  0.612709  0.485408  0.569499  0.443251   \n",
       "above_5   0.590197  0.190669  0.467855  0.336203  0.529855  0.440585   \n",
       "Y         0.302341 -0.040493  0.111111  0.051323  0.219223  0.190947   \n",
       "\n",
       "              mean  F_w_mean   above_3   above_4   above_5         Y  \n",
       "X1        0.528688  0.841993  0.139387  0.443484  0.590197  0.302341  \n",
       "X2        0.413327  0.019487  0.466674  0.276085  0.190669 -0.040493  \n",
       "X3        0.664461  0.393921  0.382278  0.612709  0.467855  0.111111  \n",
       "X4        0.522414  0.211180  0.379861  0.485408  0.336203  0.051323  \n",
       "X5        0.661435  0.732961  0.473777  0.569499  0.529855  0.219223  \n",
       "X6        0.542631  0.607012  0.287410  0.443251  0.440585  0.190947  \n",
       "mean      1.000000  0.807285  0.665069  0.846197  0.749861  0.234216  \n",
       "F_w_mean  0.807285  1.000000  0.407332  0.685802  0.735868  0.329373  \n",
       "above_3   0.665069  0.407332  1.000000  0.441063  0.186543  0.056634  \n",
       "above_4   0.846197  0.685802  0.441063  1.000000  0.498374  0.206611  \n",
       "above_5   0.749861  0.735868  0.186543  0.498374  1.000000  0.235986  \n",
       "Y         0.234216  0.329373  0.056634  0.206611  0.235986  1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total=x_t_copy.copy(deep=True)\n",
    "total[\"Y\"]=y_t[\"Y\"]\n",
    "total_corr=total.corr()\n",
    "total_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3390cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corr[\"features\"]=total_corr.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d890c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.302341</td>\n",
       "      <td>X1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X2</th>\n",
       "      <td>0.003575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.202463</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.466674</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>-0.040493</td>\n",
       "      <td>X2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3</th>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.202463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282279</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>X3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X4</th>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.282279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>0.051323</td>\n",
       "      <td>X4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X5</th>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.219223</td>\n",
       "      <td>X5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X6</th>\n",
       "      <td>0.376458</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.190947</td>\n",
       "      <td>X6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.234216</td>\n",
       "      <td>mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F_w_mean</th>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.329373</td>\n",
       "      <td>F_w_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_3</th>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.466674</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>above_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_4</th>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>0.206611</td>\n",
       "      <td>above_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_5</th>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235986</td>\n",
       "      <td>above_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0.302341</td>\n",
       "      <td>-0.040493</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.051323</td>\n",
       "      <td>0.219223</td>\n",
       "      <td>0.190947</td>\n",
       "      <td>0.234216</td>\n",
       "      <td>0.329373</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>0.206611</td>\n",
       "      <td>0.235986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                X1        X2        X3        X4        X5        X6  \\\n",
       "X1        1.000000  0.003575  0.197993  0.033369  0.337179  0.376458   \n",
       "X2        0.003575  1.000000  0.202463  0.054034  0.009242 -0.087204   \n",
       "X3        0.197993  0.202463  1.000000  0.282279  0.296808  0.186113   \n",
       "X4        0.033369  0.054034  0.282279  1.000000  0.209190  0.227200   \n",
       "X5        0.337179  0.009242  0.296808  0.209190  1.000000  0.283442   \n",
       "X6        0.376458 -0.087204  0.186113  0.227200  0.283442  1.000000   \n",
       "mean      0.528688  0.413327  0.664461  0.522414  0.661435  0.542631   \n",
       "F_w_mean  0.841993  0.019487  0.393921  0.211180  0.732961  0.607012   \n",
       "above_3   0.139387  0.466674  0.382278  0.379861  0.473777  0.287410   \n",
       "above_4   0.443484  0.276085  0.612709  0.485408  0.569499  0.443251   \n",
       "above_5   0.590197  0.190669  0.467855  0.336203  0.529855  0.440585   \n",
       "Y         0.302341 -0.040493  0.111111  0.051323  0.219223  0.190947   \n",
       "\n",
       "              mean  F_w_mean   above_3   above_4   above_5         Y  features  \n",
       "X1        0.528688  0.841993  0.139387  0.443484  0.590197  0.302341        X1  \n",
       "X2        0.413327  0.019487  0.466674  0.276085  0.190669 -0.040493        X2  \n",
       "X3        0.664461  0.393921  0.382278  0.612709  0.467855  0.111111        X3  \n",
       "X4        0.522414  0.211180  0.379861  0.485408  0.336203  0.051323        X4  \n",
       "X5        0.661435  0.732961  0.473777  0.569499  0.529855  0.219223        X5  \n",
       "X6        0.542631  0.607012  0.287410  0.443251  0.440585  0.190947        X6  \n",
       "mean      1.000000  0.807285  0.665069  0.846197  0.749861  0.234216      mean  \n",
       "F_w_mean  0.807285  1.000000  0.407332  0.685802  0.735868  0.329373  F_w_mean  \n",
       "above_3   0.665069  0.407332  1.000000  0.441063  0.186543  0.056634   above_3  \n",
       "above_4   0.846197  0.685802  0.441063  1.000000  0.498374  0.206611   above_4  \n",
       "above_5   0.749861  0.735868  0.186543  0.498374  1.000000  0.235986   above_5  \n",
       "Y         0.234216  0.329373  0.056634  0.206611  0.235986  1.000000         Y  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f8c1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X1</th>\n",
       "      <td>0.302341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X2</th>\n",
       "      <td>-0.040493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3</th>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X4</th>\n",
       "      <td>0.051323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X5</th>\n",
       "      <td>0.219223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X6</th>\n",
       "      <td>0.190947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.234216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F_w_mean</th>\n",
       "      <td>0.329373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_3</th>\n",
       "      <td>0.056634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_4</th>\n",
       "      <td>0.206611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_5</th>\n",
       "      <td>0.235986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Y\n",
       "X1        0.302341\n",
       "X2       -0.040493\n",
       "X3        0.111111\n",
       "X4        0.051323\n",
       "X5        0.219223\n",
       "X6        0.190947\n",
       "mean      0.234216\n",
       "F_w_mean  0.329373\n",
       "above_3   0.056634\n",
       "above_4   0.206611\n",
       "above_5   0.235986\n",
       "Y         1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_corr[[\"Y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b931b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F_w_mean</th>\n",
       "      <td>0.329373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X1</th>\n",
       "      <td>0.302341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_5</th>\n",
       "      <td>0.235986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.234216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X5</th>\n",
       "      <td>0.219223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_4</th>\n",
       "      <td>0.206611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X6</th>\n",
       "      <td>0.190947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3</th>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above_3</th>\n",
       "      <td>0.056634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X4</th>\n",
       "      <td>0.051323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X2</th>\n",
       "      <td>-0.040493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Y\n",
       "F_w_mean  0.329373\n",
       "X1        0.302341\n",
       "above_5   0.235986\n",
       "mean      0.234216\n",
       "X5        0.219223\n",
       "above_4   0.206611\n",
       "X6        0.190947\n",
       "X3        0.111111\n",
       "above_3   0.056634\n",
       "X4        0.051323\n",
       "X2       -0.040493"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_corr[[\"Y\"]].sort_values(by=[\"Y\"], ascending=False).drop(\"Y\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81b81064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>9.859455</td>\n",
       "      <td>0.002233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X2</td>\n",
       "      <td>0.160949</td>\n",
       "      <td>0.689158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X3</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>0.271091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X4</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.612073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X5</td>\n",
       "      <td>4.947507</td>\n",
       "      <td>0.028423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.708349</td>\n",
       "      <td>0.057039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean</td>\n",
       "      <td>5.688036</td>\n",
       "      <td>0.019005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>11.925434</td>\n",
       "      <td>0.000819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>above_3</td>\n",
       "      <td>0.315334</td>\n",
       "      <td>0.575707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>above_4</td>\n",
       "      <td>4.369989</td>\n",
       "      <td>0.039165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>above_5</td>\n",
       "      <td>5.779392</td>\n",
       "      <td>0.018095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    features    f score   p value\n",
       "0         X1   9.859455  0.002233\n",
       "1         X2   0.160949  0.689158\n",
       "2         X3   1.225000  0.271091\n",
       "3         X4   0.258824  0.612073\n",
       "4         X5   4.947507  0.028423\n",
       "5         X6   3.708349  0.057039\n",
       "6       mean   5.688036  0.019005\n",
       "7   F_w_mean  11.925434  0.000819\n",
       "8    above_3   0.315334  0.575707\n",
       "9    above_4   4.369989  0.039165\n",
       "10   above_5   5.779392  0.018095"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_score, p_value = f_classif(X=x_t_copy, y=y_t[\"Y\"])\n",
    "df_tot_f=pd.DataFrame({\"features\": x_t_copy.columns, \n",
    "                       \"f score\": f_score, \n",
    "                       \"p value\": p_value})\n",
    "df_tot_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b886661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>11.925434</td>\n",
       "      <td>0.000819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>9.859455</td>\n",
       "      <td>0.002233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>above_5</td>\n",
       "      <td>5.779392</td>\n",
       "      <td>0.018095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean</td>\n",
       "      <td>5.688036</td>\n",
       "      <td>0.019005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X5</td>\n",
       "      <td>4.947507</td>\n",
       "      <td>0.028423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>above_4</td>\n",
       "      <td>4.369989</td>\n",
       "      <td>0.039165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.708349</td>\n",
       "      <td>0.057039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X3</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>0.271091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>above_3</td>\n",
       "      <td>0.315334</td>\n",
       "      <td>0.575707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X4</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.612073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X2</td>\n",
       "      <td>0.160949</td>\n",
       "      <td>0.689158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    features    f score   p value\n",
       "7   F_w_mean  11.925434  0.000819\n",
       "0         X1   9.859455  0.002233\n",
       "10   above_5   5.779392  0.018095\n",
       "6       mean   5.688036  0.019005\n",
       "4         X5   4.947507  0.028423\n",
       "9    above_4   4.369989  0.039165\n",
       "5         X6   3.708349  0.057039\n",
       "2         X3   1.225000  0.271091\n",
       "8    above_3   0.315334  0.575707\n",
       "3         X4   0.258824  0.612073\n",
       "1         X2   0.160949  0.689158"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tot_f.sort_values(by=[\"f score\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b3085e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>9.859455</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.302341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X2</td>\n",
       "      <td>0.160949</td>\n",
       "      <td>0.689158</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.202463</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.466674</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>-0.040493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X3</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>0.271091</td>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.202463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282279</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X4</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.612073</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.282279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>0.051323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X5</td>\n",
       "      <td>4.947507</td>\n",
       "      <td>0.028423</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.219223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.708349</td>\n",
       "      <td>0.057039</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.190947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean</td>\n",
       "      <td>5.688036</td>\n",
       "      <td>0.019005</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.234216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>11.925434</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.329373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>above_3</td>\n",
       "      <td>0.315334</td>\n",
       "      <td>0.575707</td>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.466674</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.056634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>above_4</td>\n",
       "      <td>4.369989</td>\n",
       "      <td>0.039165</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>0.206611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>above_5</td>\n",
       "      <td>5.779392</td>\n",
       "      <td>0.018095</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    features    f score   p value        X1        X2        X3        X4  \\\n",
       "0         X1   9.859455  0.002233  1.000000  0.003575  0.197993  0.033369   \n",
       "1         X2   0.160949  0.689158  0.003575  1.000000  0.202463  0.054034   \n",
       "2         X3   1.225000  0.271091  0.197993  0.202463  1.000000  0.282279   \n",
       "3         X4   0.258824  0.612073  0.033369  0.054034  0.282279  1.000000   \n",
       "4         X5   4.947507  0.028423  0.337179  0.009242  0.296808  0.209190   \n",
       "5         X6   3.708349  0.057039  0.376458 -0.087204  0.186113  0.227200   \n",
       "6       mean   5.688036  0.019005  0.528688  0.413327  0.664461  0.522414   \n",
       "7   F_w_mean  11.925434  0.000819  0.841993  0.019487  0.393921  0.211180   \n",
       "8    above_3   0.315334  0.575707  0.139387  0.466674  0.382278  0.379861   \n",
       "9    above_4   4.369989  0.039165  0.443484  0.276085  0.612709  0.485408   \n",
       "10   above_5   5.779392  0.018095  0.590197  0.190669  0.467855  0.336203   \n",
       "\n",
       "          X5        X6      mean  F_w_mean   above_3   above_4   above_5  \\\n",
       "0   0.337179  0.376458  0.528688  0.841993  0.139387  0.443484  0.590197   \n",
       "1   0.009242 -0.087204  0.413327  0.019487  0.466674  0.276085  0.190669   \n",
       "2   0.296808  0.186113  0.664461  0.393921  0.382278  0.612709  0.467855   \n",
       "3   0.209190  0.227200  0.522414  0.211180  0.379861  0.485408  0.336203   \n",
       "4   1.000000  0.283442  0.661435  0.732961  0.473777  0.569499  0.529855   \n",
       "5   0.283442  1.000000  0.542631  0.607012  0.287410  0.443251  0.440585   \n",
       "6   0.661435  0.542631  1.000000  0.807285  0.665069  0.846197  0.749861   \n",
       "7   0.732961  0.607012  0.807285  1.000000  0.407332  0.685802  0.735868   \n",
       "8   0.473777  0.287410  0.665069  0.407332  1.000000  0.441063  0.186543   \n",
       "9   0.569499  0.443251  0.846197  0.685802  0.441063  1.000000  0.498374   \n",
       "10  0.529855  0.440585  0.749861  0.735868  0.186543  0.498374  1.000000   \n",
       "\n",
       "           Y  \n",
       "0   0.302341  \n",
       "1  -0.040493  \n",
       "2   0.111111  \n",
       "3   0.051323  \n",
       "4   0.219223  \n",
       "5   0.190947  \n",
       "6   0.234216  \n",
       "7   0.329373  \n",
       "8   0.056634  \n",
       "9   0.206611  \n",
       "10  0.235986  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_com=pd.merge(left=df_tot_f,right=total_corr, on=[\"features\"], how=\"left\")\n",
    "df_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d31c281c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>11.925434</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.329373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X1</td>\n",
       "      <td>9.859455</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.302341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>above_5</td>\n",
       "      <td>5.779392</td>\n",
       "      <td>0.018095</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mean</td>\n",
       "      <td>5.688036</td>\n",
       "      <td>0.019005</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.234216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X5</td>\n",
       "      <td>4.947507</td>\n",
       "      <td>0.028423</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.219223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>above_4</td>\n",
       "      <td>4.369989</td>\n",
       "      <td>0.039165</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>0.206611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.708349</td>\n",
       "      <td>0.057039</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.190947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>X3</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>0.271091</td>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.202463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282279</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>above_3</td>\n",
       "      <td>0.315334</td>\n",
       "      <td>0.575707</td>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.466674</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.056634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>X4</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.612073</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.282279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>0.051323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>X2</td>\n",
       "      <td>0.160949</td>\n",
       "      <td>0.689158</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.202463</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.466674</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>-0.040493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    features    f score   p value        X1        X2        X3        X4  \\\n",
       "0   F_w_mean  11.925434  0.000819  0.841993  0.019487  0.393921  0.211180   \n",
       "1         X1   9.859455  0.002233  1.000000  0.003575  0.197993  0.033369   \n",
       "2    above_5   5.779392  0.018095  0.590197  0.190669  0.467855  0.336203   \n",
       "3       mean   5.688036  0.019005  0.528688  0.413327  0.664461  0.522414   \n",
       "4         X5   4.947507  0.028423  0.337179  0.009242  0.296808  0.209190   \n",
       "5    above_4   4.369989  0.039165  0.443484  0.276085  0.612709  0.485408   \n",
       "6         X6   3.708349  0.057039  0.376458 -0.087204  0.186113  0.227200   \n",
       "7         X3   1.225000  0.271091  0.197993  0.202463  1.000000  0.282279   \n",
       "8    above_3   0.315334  0.575707  0.139387  0.466674  0.382278  0.379861   \n",
       "9         X4   0.258824  0.612073  0.033369  0.054034  0.282279  1.000000   \n",
       "10        X2   0.160949  0.689158  0.003575  1.000000  0.202463  0.054034   \n",
       "\n",
       "          X5        X6      mean  F_w_mean   above_3   above_4   above_5  \\\n",
       "0   0.732961  0.607012  0.807285  1.000000  0.407332  0.685802  0.735868   \n",
       "1   0.337179  0.376458  0.528688  0.841993  0.139387  0.443484  0.590197   \n",
       "2   0.529855  0.440585  0.749861  0.735868  0.186543  0.498374  1.000000   \n",
       "3   0.661435  0.542631  1.000000  0.807285  0.665069  0.846197  0.749861   \n",
       "4   1.000000  0.283442  0.661435  0.732961  0.473777  0.569499  0.529855   \n",
       "5   0.569499  0.443251  0.846197  0.685802  0.441063  1.000000  0.498374   \n",
       "6   0.283442  1.000000  0.542631  0.607012  0.287410  0.443251  0.440585   \n",
       "7   0.296808  0.186113  0.664461  0.393921  0.382278  0.612709  0.467855   \n",
       "8   0.473777  0.287410  0.665069  0.407332  1.000000  0.441063  0.186543   \n",
       "9   0.209190  0.227200  0.522414  0.211180  0.379861  0.485408  0.336203   \n",
       "10  0.009242 -0.087204  0.413327  0.019487  0.466674  0.276085  0.190669   \n",
       "\n",
       "           Y  \n",
       "0   0.329373  \n",
       "1   0.302341  \n",
       "2   0.235986  \n",
       "3   0.234216  \n",
       "4   0.219223  \n",
       "5   0.206611  \n",
       "6   0.190947  \n",
       "7   0.111111  \n",
       "8   0.056634  \n",
       "9   0.051323  \n",
       "10 -0.040493  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted_com=df_com.sort_values(by=[\"f score\", \"Y\"], ascending=False).reset_index(drop=True)\n",
    "df_sorted_com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408daa9f",
   "metadata": {},
   "source": [
    "If we have some layer in between that can handle collinearity automatically, we do not have to perform feature selection to remove high collinearity. \n",
    "We could also just run through all combination of the total features above and see what does best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3ac014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2047"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_combin=[]\n",
    "\n",
    "for n_chosen in range(1, len(df_sorted_com[\"features\"].values)+1): \n",
    "    all_combin.extend(list(itertools.combinations(df_sorted_com[\"features\"].values, n_chosen)))\n",
    "    \n",
    "len(all_combin) #This should be 2^{11}-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bab102",
   "metadata": {},
   "source": [
    "See above that there are A LOT OF combinations, it is NOT a good idea to run through all of them. But we can try to remove some: especially the ones with f score smaller than 1 and p value smaller bigger than 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95cc3f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>11.925434</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.329373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X1</td>\n",
       "      <td>9.859455</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.302341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>above_5</td>\n",
       "      <td>5.779392</td>\n",
       "      <td>0.018095</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mean</td>\n",
       "      <td>5.688036</td>\n",
       "      <td>0.019005</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.234216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X5</td>\n",
       "      <td>4.947507</td>\n",
       "      <td>0.028423</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.219223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>above_4</td>\n",
       "      <td>4.369989</td>\n",
       "      <td>0.039165</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>0.206611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.708349</td>\n",
       "      <td>0.057039</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.190947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features    f score   p value        X1        X2        X3        X4  \\\n",
       "0  F_w_mean  11.925434  0.000819  0.841993  0.019487  0.393921  0.211180   \n",
       "1        X1   9.859455  0.002233  1.000000  0.003575  0.197993  0.033369   \n",
       "2   above_5   5.779392  0.018095  0.590197  0.190669  0.467855  0.336203   \n",
       "3      mean   5.688036  0.019005  0.528688  0.413327  0.664461  0.522414   \n",
       "4        X5   4.947507  0.028423  0.337179  0.009242  0.296808  0.209190   \n",
       "5   above_4   4.369989  0.039165  0.443484  0.276085  0.612709  0.485408   \n",
       "6        X6   3.708349  0.057039  0.376458 -0.087204  0.186113  0.227200   \n",
       "\n",
       "         X5        X6      mean  F_w_mean   above_3   above_4   above_5  \\\n",
       "0  0.732961  0.607012  0.807285  1.000000  0.407332  0.685802  0.735868   \n",
       "1  0.337179  0.376458  0.528688  0.841993  0.139387  0.443484  0.590197   \n",
       "2  0.529855  0.440585  0.749861  0.735868  0.186543  0.498374  1.000000   \n",
       "3  0.661435  0.542631  1.000000  0.807285  0.665069  0.846197  0.749861   \n",
       "4  1.000000  0.283442  0.661435  0.732961  0.473777  0.569499  0.529855   \n",
       "5  0.569499  0.443251  0.846197  0.685802  0.441063  1.000000  0.498374   \n",
       "6  0.283442  1.000000  0.542631  0.607012  0.287410  0.443251  0.440585   \n",
       "\n",
       "          Y  \n",
       "0  0.329373  \n",
       "1  0.302341  \n",
       "2  0.235986  \n",
       "3  0.234216  \n",
       "4  0.219223  \n",
       "5  0.206611  \n",
       "6  0.190947  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted_sel=df_sorted_com[(df_sorted_com[\"f score\"]>1)&(df_sorted_com[\"p value\"]<0.1)]\n",
    "df_sorted_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1056e3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_combin=[]\n",
    "\n",
    "for n_chosen in range(1, len(df_sorted_sel[\"features\"].values)+1): \n",
    "    all_combin.extend(list(itertools.combinations(df_sorted_sel[\"features\"].values, n_chosen)))\n",
    "    \n",
    "len(all_combin) #This should be 2^{7}-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3108658d",
   "metadata": {},
   "source": [
    "Now it is much more workable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699ac90",
   "metadata": {},
   "source": [
    "Let's make this part of the pipelin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c9fd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from proj_mod import training\n",
    "importlib.reload(training);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4b931",
   "metadata": {},
   "source": [
    "Behold, magic: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b42ca8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>10.289766</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>15.470365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>12.703944</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>14.629468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>10.289766</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>12.615471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>11.586720</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.716028</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>15.604569</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>13.773472</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    X1  X2  X3  X4  X5  X6      mean   F_w_mean   above_3   above_4   above_5\n",
       "28   3   3   1   3   3   4  2.833333  10.289766  0.833333  0.166667  0.000000\n",
       "46   5   3   3   4   4   5  4.000000  15.470365  1.000000  0.666667  0.333333\n",
       "67   4   1   1   3   4   4  2.833333  12.703944  0.666667  0.500000  0.000000\n",
       "3    5   4   3   3   3   5  3.833333  14.629468  1.000000  0.500000  0.333333\n",
       "27   3   3   1   3   3   4  2.833333  10.289766  0.833333  0.166667  0.000000\n",
       "..  ..  ..  ..  ..  ..  ..       ...        ...       ...       ...       ...\n",
       "57   4   4   4   4   3   4  3.833333  12.615471  1.000000  0.833333  0.000000\n",
       "39   4   4   3   4   2   4  3.500000  11.586720  0.833333  0.666667  0.000000\n",
       "94   3   1   3   2   1   2  2.000000   7.716028  0.333333  0.000000  0.000000\n",
       "53   5   2   4   3   4   5  3.833333  15.604569  0.833333  0.666667  0.333333\n",
       "31   4   1   3   4   4   5  3.500000  13.773472  0.833333  0.666667  0.166667\n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_creator=training.data_creator()\n",
    "y_t_arr=np.ravel(y_t.values)\n",
    "data_creator.fit(X=x_t,y=y_t_arr)\n",
    "x_t_c=data_creator.transform(X=x_t)\n",
    "x_t_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "012a3bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>9.916652</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>16.703282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>16.391736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>15.577744</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>16.472291</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1  X2  X3  X4  X5  X6      mean   F_w_mean   above_3   above_4   above_5\n",
       "0     3   3   3   4   2   4  3.166667   9.916652  0.833333  0.333333  0.000000\n",
       "115   5   3   5   4   5   5  4.500000  16.703282  1.000000  0.833333  0.666667\n",
       "5     5   5   3   5   5   5  4.666667  16.391736  1.000000  0.833333  0.833333\n",
       "17    5   1   4   3   4   5  3.666667  15.577744  0.833333  0.666667  0.333333\n",
       "108   5   2   4   4   5   5  4.166667  16.472291  0.833333  0.833333  0.500000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_v_c=data_creator.transform(X=x_v)\n",
    "x_v_c.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e64b2ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>10.289766</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>15.470365</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>12.703944</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>14.629468</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>10.289766</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>12.615471</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>11.586720</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.716028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>15.604569</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>13.773472</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    X1  X5  X6      mean   F_w_mean   above_4   above_5\n",
       "28   3   3   4  2.833333  10.289766  0.166667  0.000000\n",
       "46   5   4   5  4.000000  15.470365  0.666667  0.333333\n",
       "67   4   4   4  2.833333  12.703944  0.500000  0.000000\n",
       "3    5   3   5  3.833333  14.629468  0.500000  0.333333\n",
       "27   3   3   4  2.833333  10.289766  0.166667  0.000000\n",
       "..  ..  ..  ..       ...        ...       ...       ...\n",
       "57   4   3   4  3.833333  12.615471  0.833333  0.000000\n",
       "39   4   2   4  3.500000  11.586720  0.666667  0.000000\n",
       "94   3   1   2  2.000000   7.716028  0.000000  0.000000\n",
       "53   5   4   5  3.833333  15.604569  0.666667  0.333333\n",
       "31   4   4   5  3.500000  13.773472  0.666667  0.166667\n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_selector=training.data_selector()\n",
    "y_t_arr=np.ravel(y_t.values)\n",
    "data_selector.fit(X=x_t_c,y=y_t_arr)\n",
    "x_t_s=data_selector.transform(X=x_t_c)\n",
    "x_t_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6c966d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>16.719595</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>13.155414</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>14.575818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>16.472291</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>9.916652</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1  X5  X6      mean   F_w_mean   above_4   above_5\n",
       "74    5   5   5  4.500000  16.719595  0.833333  0.833333\n",
       "64    4   4   4  3.333333  13.155414  0.666667  0.000000\n",
       "102   5   3   5  3.500000  14.575818  0.333333  0.333333\n",
       "108   5   5   5  4.166667  16.472291  0.833333  0.500000\n",
       "0     3   2   4  3.166667   9.916652  0.333333  0.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_v_s=data_selector.transform(X=x_v_c)\n",
    "x_v_s.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cf5027f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>9.859455</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.007137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X5</td>\n",
       "      <td>4.947507</td>\n",
       "      <td>0.028423</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>-0.055694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.708349</td>\n",
       "      <td>0.057039</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>-0.161374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean</td>\n",
       "      <td>5.688036</td>\n",
       "      <td>0.019005</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>-0.125391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>11.925434</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>-0.062969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>above_4</td>\n",
       "      <td>4.369989</td>\n",
       "      <td>0.039165</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>-0.031160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>above_5</td>\n",
       "      <td>5.779392</td>\n",
       "      <td>0.018095</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.047054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    features    f score   p value        X1        X2        X3        X4  \\\n",
       "0         X1   9.859455  0.002233  1.000000  0.003575  0.197993  0.033369   \n",
       "4         X5   4.947507  0.028423  0.337179  0.009242  0.296808  0.209190   \n",
       "5         X6   3.708349  0.057039  0.376458 -0.087204  0.186113  0.227200   \n",
       "6       mean   5.688036  0.019005  0.528688  0.413327  0.664461  0.522414   \n",
       "7   F_w_mean  11.925434  0.000819  0.841993  0.019487  0.393921  0.211180   \n",
       "9    above_4   4.369989  0.039165  0.443484  0.276085  0.612709  0.485408   \n",
       "10   above_5   5.779392  0.018095  0.590197  0.190669  0.467855  0.336203   \n",
       "\n",
       "          X5        X6      mean  F_w_mean   above_3   above_4   above_5  \\\n",
       "0   0.337179  0.376458  0.528688  0.841993  0.139387  0.443484  0.590197   \n",
       "4   1.000000  0.283442  0.661435  0.732961  0.473777  0.569499  0.529855   \n",
       "5   0.283442  1.000000  0.542631  0.607012  0.287410  0.443251  0.440585   \n",
       "6   0.661435  0.542631  1.000000  0.807285  0.665069  0.846197  0.749861   \n",
       "7   0.732961  0.607012  0.807285  1.000000  0.407332  0.685802  0.735868   \n",
       "9   0.569499  0.443251  0.846197  0.685802  0.441063  1.000000  0.498374   \n",
       "10  0.529855  0.440585  0.749861  0.735868  0.186543  0.498374  1.000000   \n",
       "\n",
       "           Y  \n",
       "0   0.007137  \n",
       "4  -0.055694  \n",
       "5  -0.161374  \n",
       "6  -0.125391  \n",
       "7  -0.062969  \n",
       "9  -0.031160  \n",
       "10 -0.047054  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_selector.sel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d704dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>9.859455</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.007137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X2</td>\n",
       "      <td>0.160949</td>\n",
       "      <td>0.689158</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.202463</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.466674</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>-0.115609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X3</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>0.271091</td>\n",
       "      <td>0.197993</td>\n",
       "      <td>0.202463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282279</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>-0.043774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X4</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.612073</td>\n",
       "      <td>0.033369</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.282279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>-0.081541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X5</td>\n",
       "      <td>4.947507</td>\n",
       "      <td>0.028423</td>\n",
       "      <td>0.337179</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.296808</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>-0.055694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.708349</td>\n",
       "      <td>0.057039</td>\n",
       "      <td>0.376458</td>\n",
       "      <td>-0.087204</td>\n",
       "      <td>0.186113</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.283442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>-0.161374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean</td>\n",
       "      <td>5.688036</td>\n",
       "      <td>0.019005</td>\n",
       "      <td>0.528688</td>\n",
       "      <td>0.413327</td>\n",
       "      <td>0.664461</td>\n",
       "      <td>0.522414</td>\n",
       "      <td>0.661435</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>-0.125391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>11.925434</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.841993</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.393921</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.732961</td>\n",
       "      <td>0.607012</td>\n",
       "      <td>0.807285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>-0.062969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>above_3</td>\n",
       "      <td>0.315334</td>\n",
       "      <td>0.575707</td>\n",
       "      <td>0.139387</td>\n",
       "      <td>0.466674</td>\n",
       "      <td>0.382278</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.665069</td>\n",
       "      <td>0.407332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>-0.227185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>above_4</td>\n",
       "      <td>4.369989</td>\n",
       "      <td>0.039165</td>\n",
       "      <td>0.443484</td>\n",
       "      <td>0.276085</td>\n",
       "      <td>0.612709</td>\n",
       "      <td>0.485408</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.443251</td>\n",
       "      <td>0.846197</td>\n",
       "      <td>0.685802</td>\n",
       "      <td>0.441063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>-0.031160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>above_5</td>\n",
       "      <td>5.779392</td>\n",
       "      <td>0.018095</td>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.190669</td>\n",
       "      <td>0.467855</td>\n",
       "      <td>0.336203</td>\n",
       "      <td>0.529855</td>\n",
       "      <td>0.440585</td>\n",
       "      <td>0.749861</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.186543</td>\n",
       "      <td>0.498374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.047054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    features    f score   p value        X1        X2        X3        X4  \\\n",
       "0         X1   9.859455  0.002233  1.000000  0.003575  0.197993  0.033369   \n",
       "1         X2   0.160949  0.689158  0.003575  1.000000  0.202463  0.054034   \n",
       "2         X3   1.225000  0.271091  0.197993  0.202463  1.000000  0.282279   \n",
       "3         X4   0.258824  0.612073  0.033369  0.054034  0.282279  1.000000   \n",
       "4         X5   4.947507  0.028423  0.337179  0.009242  0.296808  0.209190   \n",
       "5         X6   3.708349  0.057039  0.376458 -0.087204  0.186113  0.227200   \n",
       "6       mean   5.688036  0.019005  0.528688  0.413327  0.664461  0.522414   \n",
       "7   F_w_mean  11.925434  0.000819  0.841993  0.019487  0.393921  0.211180   \n",
       "8    above_3   0.315334  0.575707  0.139387  0.466674  0.382278  0.379861   \n",
       "9    above_4   4.369989  0.039165  0.443484  0.276085  0.612709  0.485408   \n",
       "10   above_5   5.779392  0.018095  0.590197  0.190669  0.467855  0.336203   \n",
       "\n",
       "          X5        X6      mean  F_w_mean   above_3   above_4   above_5  \\\n",
       "0   0.337179  0.376458  0.528688  0.841993  0.139387  0.443484  0.590197   \n",
       "1   0.009242 -0.087204  0.413327  0.019487  0.466674  0.276085  0.190669   \n",
       "2   0.296808  0.186113  0.664461  0.393921  0.382278  0.612709  0.467855   \n",
       "3   0.209190  0.227200  0.522414  0.211180  0.379861  0.485408  0.336203   \n",
       "4   1.000000  0.283442  0.661435  0.732961  0.473777  0.569499  0.529855   \n",
       "5   0.283442  1.000000  0.542631  0.607012  0.287410  0.443251  0.440585   \n",
       "6   0.661435  0.542631  1.000000  0.807285  0.665069  0.846197  0.749861   \n",
       "7   0.732961  0.607012  0.807285  1.000000  0.407332  0.685802  0.735868   \n",
       "8   0.473777  0.287410  0.665069  0.407332  1.000000  0.441063  0.186543   \n",
       "9   0.569499  0.443251  0.846197  0.685802  0.441063  1.000000  0.498374   \n",
       "10  0.529855  0.440585  0.749861  0.735868  0.186543  0.498374  1.000000   \n",
       "\n",
       "           Y  \n",
       "0   0.007137  \n",
       "1  -0.115609  \n",
       "2  -0.043774  \n",
       "3  -0.081541  \n",
       "4  -0.055694  \n",
       "5  -0.161374  \n",
       "6  -0.125391  \n",
       "7  -0.062969  \n",
       "8  -0.227185  \n",
       "9  -0.031160  \n",
       "10 -0.047054  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_selector.total_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a71028",
   "metadata": {},
   "source": [
    "Let's go through a quick loop being going further: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d5e6c",
   "metadata": {},
   "source": [
    "## Produce model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820494f",
   "metadata": {},
   "source": [
    "### Using all of raw features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4281e62f",
   "metadata": {},
   "source": [
    "We first, use the whole dataset to decide on the ranking, this will be then used as a fixed list throughout training to avoid data leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5acaed35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X3', 'X5', 'X6', 'mean', 'F_w_mean', 'above_4', 'above_5'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_pipe=Pipeline([(\"DataCreater\", training.data_creator()),(\"DataSelector\",training.data_selector())])\n",
    "tar_arr=np.ravel(tar.values)\n",
    "eva_pipe.fit(X=feat,y=tar)\n",
    "eva_out=eva_pipe.transform(X=feat)\n",
    "eva_out.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c83c6e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>10.561708</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059797</td>\n",
       "      <td>0.283358</td>\n",
       "      <td>0.087541</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>0.607460</td>\n",
       "      <td>0.834641</td>\n",
       "      <td>0.266199</td>\n",
       "      <td>0.492355</td>\n",
       "      <td>0.604855</td>\n",
       "      <td>0.280160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X3</td>\n",
       "      <td>2.886959</td>\n",
       "      <td>0.091807</td>\n",
       "      <td>0.283358</td>\n",
       "      <td>0.184129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.302618</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>0.203750</td>\n",
       "      <td>0.676149</td>\n",
       "      <td>0.532371</td>\n",
       "      <td>0.442280</td>\n",
       "      <td>0.638649</td>\n",
       "      <td>0.481689</td>\n",
       "      <td>0.150838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X5</td>\n",
       "      <td>6.582716</td>\n",
       "      <td>0.011488</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>0.039996</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>0.293115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.320195</td>\n",
       "      <td>0.712786</td>\n",
       "      <td>0.806779</td>\n",
       "      <td>0.491804</td>\n",
       "      <td>0.616787</td>\n",
       "      <td>0.586695</td>\n",
       "      <td>0.224522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.586849</td>\n",
       "      <td>0.060568</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>-0.062205</td>\n",
       "      <td>0.203750</td>\n",
       "      <td>0.215888</td>\n",
       "      <td>0.320195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540096</td>\n",
       "      <td>0.574523</td>\n",
       "      <td>0.261704</td>\n",
       "      <td>0.458477</td>\n",
       "      <td>0.490605</td>\n",
       "      <td>0.167669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean</td>\n",
       "      <td>7.306094</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.607460</td>\n",
       "      <td>0.426097</td>\n",
       "      <td>0.676149</td>\n",
       "      <td>0.557803</td>\n",
       "      <td>0.712786</td>\n",
       "      <td>0.540096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869373</td>\n",
       "      <td>0.687659</td>\n",
       "      <td>0.848710</td>\n",
       "      <td>0.773920</td>\n",
       "      <td>0.235885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>12.615311</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.834641</td>\n",
       "      <td>0.078909</td>\n",
       "      <td>0.532371</td>\n",
       "      <td>0.298662</td>\n",
       "      <td>0.806779</td>\n",
       "      <td>0.574523</td>\n",
       "      <td>0.869373</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498831</td>\n",
       "      <td>0.743851</td>\n",
       "      <td>0.761327</td>\n",
       "      <td>0.303878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>above_4</td>\n",
       "      <td>7.194813</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>0.492355</td>\n",
       "      <td>0.268810</td>\n",
       "      <td>0.638649</td>\n",
       "      <td>0.521454</td>\n",
       "      <td>0.616787</td>\n",
       "      <td>0.458477</td>\n",
       "      <td>0.848710</td>\n",
       "      <td>0.743851</td>\n",
       "      <td>0.465645</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.546995</td>\n",
       "      <td>0.234181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>above_5</td>\n",
       "      <td>6.520675</td>\n",
       "      <td>0.011874</td>\n",
       "      <td>0.604855</td>\n",
       "      <td>0.215269</td>\n",
       "      <td>0.481689</td>\n",
       "      <td>0.389949</td>\n",
       "      <td>0.586695</td>\n",
       "      <td>0.490605</td>\n",
       "      <td>0.773920</td>\n",
       "      <td>0.761327</td>\n",
       "      <td>0.229256</td>\n",
       "      <td>0.546995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.223515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    features    f score   p value        X1        X2        X3        X4  \\\n",
       "0         X1  10.561708  0.001486  1.000000  0.059797  0.283358  0.087541   \n",
       "2         X3   2.886959  0.091807  0.283358  0.184129  1.000000  0.302618   \n",
       "4         X5   6.582716  0.011488  0.432772  0.039996  0.358397  0.293115   \n",
       "5         X6   3.586849  0.060568  0.411873 -0.062205  0.203750  0.215888   \n",
       "6       mean   7.306094  0.007836  0.607460  0.426097  0.676149  0.557803   \n",
       "7   F_w_mean  12.615311  0.000542  0.834641  0.078909  0.532371  0.298662   \n",
       "9    above_4   7.194813  0.008308  0.492355  0.268810  0.638649  0.521454   \n",
       "10   above_5   6.520675  0.011874  0.604855  0.215269  0.481689  0.389949   \n",
       "\n",
       "          X5        X6      mean  F_w_mean   above_3   above_4   above_5  \\\n",
       "0   0.432772  0.411873  0.607460  0.834641  0.266199  0.492355  0.604855   \n",
       "2   0.358397  0.203750  0.676149  0.532371  0.442280  0.638649  0.481689   \n",
       "4   1.000000  0.320195  0.712786  0.806779  0.491804  0.616787  0.586695   \n",
       "5   0.320195  1.000000  0.540096  0.574523  0.261704  0.458477  0.490605   \n",
       "6   0.712786  0.540096  1.000000  0.869373  0.687659  0.848710  0.773920   \n",
       "7   0.806779  0.574523  0.869373  1.000000  0.498831  0.743851  0.761327   \n",
       "9   0.616787  0.458477  0.848710  0.743851  0.465645  1.000000  0.546995   \n",
       "10  0.586695  0.490605  0.773920  0.761327  0.229256  0.546995  1.000000   \n",
       "\n",
       "           Y  \n",
       "0   0.280160  \n",
       "2   0.150838  \n",
       "4   0.224522  \n",
       "5   0.167669  \n",
       "6   0.235885  \n",
       "7   0.303878  \n",
       "9   0.234181  \n",
       "10  0.223515  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_pipe[\"DataSelector\"].sel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7318c56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.45402181257102336 and std 0.30443350352074683.\n",
      "Test for 1 neighbors concluded with acc mean 0.5462153846153845 acc std 0.12220516651064098 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.12252066003550383 and std 0.24894080469829405.\n",
      "Test for 2 neighbors concluded with acc mean 0.4808461538461538 acc std 0.07886333771470086 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.3265158200685416 and std 0.3302440522183169.\n",
      "Test for 3 neighbors concluded with acc mean 0.5317846153846154 acc std 0.11889774235070519 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.21675516519812857 and std 0.31606520952452066.\n",
      "Test for 4 neighbors concluded with acc mean 0.5091076923076923 acc std 0.10496991510391478 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.5069271815530318 and std 0.2831582298568987.\n",
      "Test for 5 neighbors concluded with acc mean 0.5814461538461538 acc std 0.13048889771178046 and sureness of beating 73% 0.12.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.36623255269130917 and std 0.3256472661237853.\n",
      "Test for 6 neighbors concluded with acc mean 0.5442153846153845 acc std 0.11374415805272928 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.5672394823187776 and std 0.24163369219833494.\n",
      "Test for 7 neighbors concluded with acc mean 0.6000923076923077 acc std 0.09678007032356568 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.4820656302952504 and std 0.2952424554859988.\n",
      "Test for 8 neighbors concluded with acc mean 0.5788615384615384 acc std 0.11546307486293934 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.6167570992373154 and std 0.18638068786761408.\n",
      "Test for 9 neighbors concluded with acc mean 0.6199538461538462 acc std 0.10011034267276868 and sureness of beating 73% 0.12.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.5369852377115982 and std 0.26889685674212144.\n",
      "Test for 10 neighbors concluded with acc mean 0.6002923076923077 acc std 0.10934094010676858 and sureness of beating 73% 0.12.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.6441385627132555 and std 0.1426414225790615.\n",
      "Test for 11 neighbors concluded with acc mean 0.6329538461538461 acc std 0.09205482801541015 and sureness of beating 73% 0.14.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.5938630127942914 and std 0.21452994103929743.\n",
      "Test for 12 neighbors concluded with acc mean 0.6188 acc std 0.0948128221423851 and sureness of beating 73% 0.12.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.6547614733882433 and std 0.10628016557995709.\n",
      "Test for 13 neighbors concluded with acc mean 0.6342615384615384 acc std 0.08389705018405474 and sureness of beating 73% 0.13.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.6453181152836155 and std 0.14316079896185738.\n",
      "Test for 14 neighbors concluded with acc mean 0.6384461538461538 acc std 0.08977591959787977 and sureness of beating 73% 0.15.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1.\n",
      "This test has f1 mean 0.6564431481949182 and std 0.10673986294376278.\n",
      "Test for 15 neighbors concluded with acc mean 0.6382615384615383 acc std 0.08311812511055269 and sureness of beating 73% 0.13.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.39625079511535644 and std 0.22062215300150118.\n",
      "Test for 1 neighbors concluded with acc mean 0.48004615384615384 acc std 0.08620148545515069 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.1811466736864637 and std 0.2089947713634962.\n",
      "Test for 2 neighbors concluded with acc mean 0.484523076923077 acc std 0.05922410352103122 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.4357299119752453 and std 0.20058463018714937.\n",
      "Test for 3 neighbors concluded with acc mean 0.5225384615384615 acc std 0.08681269369730586 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.31755479381652835 and std 0.21341356902956993.\n",
      "Test for 4 neighbors concluded with acc mean 0.5013846153846154 acc std 0.08358349827227832 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.455358145773005 and std 0.20468448498391748.\n",
      "Test for 5 neighbors concluded with acc mean 0.5240615384615385 acc std 0.08471812503861953 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.3857786543965462 and std 0.21109364149273485.\n",
      "Test for 6 neighbors concluded with acc mean 0.5142615384615385 acc std 0.07812017899917988 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.4979426224840633 and std 0.1686860606413737.\n",
      "Test for 7 neighbors concluded with acc mean 0.5327692307692309 acc std 0.08508944128240284 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.42651928381734083 and std 0.18577670216987763.\n",
      "Test for 8 neighbors concluded with acc mean 0.5219076923076924 acc std 0.07128412024298658 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.5065921269800547 and std 0.1689332499405205.\n",
      "Test for 9 neighbors concluded with acc mean 0.5302769230769232 acc std 0.08433123758807347 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.47213112840948207 and std 0.16923686019949305.\n",
      "Test for 10 neighbors concluded with acc mean 0.5288307692307693 acc std 0.07947896003822268 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.5291500996797923 and std 0.13107731530702554.\n",
      "Test for 11 neighbors concluded with acc mean 0.5339076923076923 acc std 0.07247571966409441 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.5031575862983881 and std 0.13392690745042737.\n",
      "Test for 12 neighbors concluded with acc mean 0.5424923076923077 acc std 0.07392616818958808 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.5577871223576 and std 0.10030111431080713.\n",
      "Test for 13 neighbors concluded with acc mean 0.5454461538461539 acc std 0.07363091066880241 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.5464615692752024 and std 0.10823205799295627.\n",
      "Test for 14 neighbors concluded with acc mean 0.5662461538461538 acc std 0.07447546595749369 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3.\n",
      "This test has f1 mean 0.5575276555892221 and std 0.10901339590411267.\n",
      "Test for 15 neighbors concluded with acc mean 0.5485846153846154 acc std 0.07976420279384279 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.47994116956476723 and std 0.21798374229046813.\n",
      "Test for 1 neighbors concluded with acc mean 0.5232615384615386 acc std 0.0934081278219153 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.35689699441334083 and std 0.22018676277643492.\n",
      "Test for 2 neighbors concluded with acc mean 0.5152307692307694 acc std 0.08901259800472233 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.4665819267686221 and std 0.18365677193073474.\n",
      "Test for 3 neighbors concluded with acc mean 0.5221538461538462 acc std 0.09552148988361624 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.3594510351375408 and std 0.2282364249697808.\n",
      "Test for 4 neighbors concluded with acc mean 0.5229384615384616 acc std 0.0875291586242405 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.4283730327185085 and std 0.20576934705752792.\n",
      "Test for 5 neighbors concluded with acc mean 0.5264615384615385 acc std 0.08817948345151012 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.35946411675851125 and std 0.21371720327381258.\n",
      "Test for 6 neighbors concluded with acc mean 0.5164923076923077 acc std 0.09181640543874668 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.43653492384628534 and std 0.17772107378428378.\n",
      "Test for 7 neighbors concluded with acc mean 0.5327538461538461 acc std 0.09327500727552114 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.38807118775973587 and std 0.18862509767349747.\n",
      "Test for 8 neighbors concluded with acc mean 0.530723076923077 acc std 0.09341247968524734 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.4459117411968765 and std 0.1464723435849209.\n",
      "Test for 9 neighbors concluded with acc mean 0.5346307692307692 acc std 0.09919617643900568 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.42243008954223105 and std 0.15779684059368215.\n",
      "Test for 10 neighbors concluded with acc mean 0.5371230769230769 acc std 0.10047170522239511 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.44895640431814354 and std 0.13980305489802008.\n",
      "Test for 11 neighbors concluded with acc mean 0.5256000000000001 acc std 0.10041629797701308 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.43423734218449517 and std 0.13504057142360937.\n",
      "Test for 12 neighbors concluded with acc mean 0.5410461538461538 acc std 0.09112365744824756 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.45798644753960605 and std 0.13546503158923004.\n",
      "Test for 13 neighbors concluded with acc mean 0.527076923076923 acc std 0.0951498351247145 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.4446008525132686 and std 0.13767897514222407.\n",
      "Test for 14 neighbors concluded with acc mean 0.5390153846153848 acc std 0.09136610017044085 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X5.\n",
      "This test has f1 mean 0.47026990486851017 and std 0.14003375402107465.\n",
      "Test for 15 neighbors concluded with acc mean 0.531123076923077 acc std 0.08971667567119906 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.5287475095147333 and std 0.22949550484920564.\n",
      "Test for 1 neighbors concluded with acc mean 0.5335846153846154 acc std 0.09170083981986556 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.21873102198054434 and std 0.27658609530356854.\n",
      "Test for 2 neighbors concluded with acc mean 0.4880615384615385 acc std 0.06915110630321528 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.49027686477602006 and std 0.26431899767034184.\n",
      "Test for 3 neighbors concluded with acc mean 0.5344615384615384 acc std 0.10223212941156838 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.3329895444594965 and std 0.30039355523888456.\n",
      "Test for 4 neighbors concluded with acc mean 0.5168 acc std 0.08635023769681158 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.5140203541915597 and std 0.2396091982371371.\n",
      "Test for 5 neighbors concluded with acc mean 0.5453230769230769 acc std 0.09266812196436468 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.37338840476455004 and std 0.27672334459947856.\n",
      "Test for 6 neighbors concluded with acc mean 0.5155846153846153 acc std 0.08565063342391088 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.5388937763703661 and std 0.21658105833901095.\n",
      "Test for 7 neighbors concluded with acc mean 0.5506 acc std 0.08732145469301425 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.46979407944932866 and std 0.2568088309373226.\n",
      "Test for 8 neighbors concluded with acc mean 0.5361692307692307 acc std 0.08783806914164577 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.5953675946265222 and std 0.1743609361851565.\n",
      "Test for 9 neighbors concluded with acc mean 0.5654307692307691 acc std 0.09334023910772027 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.5510584631337556 and std 0.19552901736166897.\n",
      "Test for 10 neighbors concluded with acc mean 0.5476 acc std 0.09088722968992982 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.6225782752994504 and std 0.14837254121809257.\n",
      "Test for 11 neighbors concluded with acc mean 0.5715230769230769 acc std 0.082697851692509 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.5986581036626548 and std 0.15834874752025896.\n",
      "Test for 12 neighbors concluded with acc mean 0.5615692307692308 acc std 0.08269655946135583 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.665870750756817 and std 0.10729741382683651.\n",
      "Test for 13 neighbors concluded with acc mean 0.5849076923076924 acc std 0.08170337314598433 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.6370993650604764 and std 0.13096710660204255.\n",
      "Test for 14 neighbors concluded with acc mean 0.5701384615384616 acc std 0.08128351871198233 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X6.\n",
      "This test has f1 mean 0.67260560163436 and std 0.10515069763494538.\n",
      "Test for 15 neighbors concluded with acc mean 0.5868923076923077 acc std 0.08268324959333426 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.5504033108468238 and std 0.12884398807139497.\n",
      "Test for 1 neighbors concluded with acc mean 0.5393692307692307 acc std 0.09406056087662903 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.3626072091775551 and std 0.1476742875833923.\n",
      "Test for 2 neighbors concluded with acc mean 0.5075076923076923 acc std 0.08049941158960226 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.5504388782660192 and std 0.13916765188424743.\n",
      "Test for 3 neighbors concluded with acc mean 0.5470461538461538 acc std 0.09542038535254616 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.42546809780836986 and std 0.16347014188545117.\n",
      "Test for 4 neighbors concluded with acc mean 0.5236461538461539 acc std 0.08402863839790525 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.594848750114539 and std 0.12909375084040406.\n",
      "Test for 5 neighbors concluded with acc mean 0.5671538461538461 acc std 0.10023248124165134 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.5240142323991234 and std 0.1465441528530846.\n",
      "Test for 6 neighbors concluded with acc mean 0.5484923076923077 acc std 0.09242876162932921 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.6241154839974051 and std 0.1124838799763191.\n",
      "Test for 7 neighbors concluded with acc mean 0.5835384615384617 acc std 0.09536190796709272 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.5690492437133606 and std 0.13704661332898702.\n",
      "Test for 8 neighbors concluded with acc mean 0.5708000000000001 acc std 0.0997869873291938 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.6354437875322863 and std 0.11105040064502188.\n",
      "Test for 9 neighbors concluded with acc mean 0.5927076923076923 acc std 0.10270371472226056 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.589695408006121 and std 0.1316905923507876.\n",
      "Test for 10 neighbors concluded with acc mean 0.5806923076923077 acc std 0.09722623528864197 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.6458865141509207 and std 0.10221361352001007.\n",
      "Test for 11 neighbors concluded with acc mean 0.6096923076923076 acc std 0.09635965666165827 and sureness of beating 73% 0.12.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.6197883530542534 and std 0.10696828015340056.\n",
      "Test for 12 neighbors concluded with acc mean 0.5971230769230769 acc std 0.09548125501464462 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.6524935336072665 and std 0.08927063758572076.\n",
      "Test for 13 neighbors concluded with acc mean 0.5998769230769231 acc std 0.09522484882802139 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.6175048539394895 and std 0.10217411685022026.\n",
      "Test for 14 neighbors concluded with acc mean 0.5877076923076924 acc std 0.09636948005665764 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features mean.\n",
      "This test has f1 mean 0.6488566046453138 and std 0.09041463690426085.\n",
      "Test for 15 neighbors concluded with acc mean 0.5866 acc std 0.09473533234895137 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.6050576953887166 and std 0.09586024471592089.\n",
      "Test for 1 neighbors concluded with acc mean 0.5636769230769231 acc std 0.09271088738143571 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.4545127145200909 and std 0.1289105336323425.\n",
      "Test for 2 neighbors concluded with acc mean 0.5272307692307693 acc std 0.0836052997952053 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.623869203601886 and std 0.09498101921778648.\n",
      "Test for 3 neighbors concluded with acc mean 0.5838153846153847 acc std 0.08793824621817856 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.5328894498687854 and std 0.11600691287667246.\n",
      "Test for 4 neighbors concluded with acc mean 0.5580461538461539 acc std 0.08911527140525992 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.6282505205346056 and std 0.09754906033342643.\n",
      "Test for 5 neighbors concluded with acc mean 0.585923076923077 acc std 0.09476233116742129 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.5752067651410206 and std 0.12020274292782325.\n",
      "Test for 6 neighbors concluded with acc mean 0.5775384615384617 acc std 0.09949767325042591 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.6207697457854651 and std 0.10446038382574457.\n",
      "Test for 7 neighbors concluded with acc mean 0.5775230769230769 acc std 0.10451236896306672 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.5760060654388721 and std 0.11886300370051552.\n",
      "Test for 8 neighbors concluded with acc mean 0.5730923076923077 acc std 0.10238285151344276 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.6138163841735157 and std 0.10797960592715082.\n",
      "Test for 9 neighbors concluded with acc mean 0.5686923076923077 acc std 0.10760796147488838 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.5830070174780567 and std 0.10875777901405836.\n",
      "Test for 10 neighbors concluded with acc mean 0.5723076923076923 acc std 0.09917291097970932 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.6103905044709856 and std 0.09550809792369248.\n",
      "Test for 11 neighbors concluded with acc mean 0.5663846153846154 acc std 0.09288903477302954 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.5842565523022758 and std 0.10882815133055952.\n",
      "Test for 12 neighbors concluded with acc mean 0.577123076923077 acc std 0.09826077007082348 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.6077246075292566 and std 0.09484514397607957.\n",
      "Test for 13 neighbors concluded with acc mean 0.5691538461538461 acc std 0.09065321790846569 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.5815925214789581 and std 0.11031063838074374.\n",
      "Test for 14 neighbors concluded with acc mean 0.5750000000000001 acc std 0.0957554515882816 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features F_w_mean.\n",
      "This test has f1 mean 0.6049799829427918 and std 0.09385583071963341.\n",
      "Test for 15 neighbors concluded with acc mean 0.5668 acc std 0.08868538367795738 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.5052841176700759 and std 0.17594675367909032.\n",
      "Test for 1 neighbors concluded with acc mean 0.5376461538461539 acc std 0.10029482102412046 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.31779454372209254 and std 0.18480372821087854.\n",
      "Test for 2 neighbors concluded with acc mean 0.4987076923076923 acc std 0.08717735985338118 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.4787519788981755 and std 0.17704320206438973.\n",
      "Test for 3 neighbors concluded with acc mean 0.5034769230769229 acc std 0.0937956030308937 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.4045143779897027 and std 0.18904996535037433.\n",
      "Test for 4 neighbors concluded with acc mean 0.4954615384615385 acc std 0.0917205364394132 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.5571463792195994 and std 0.184172403384728.\n",
      "Test for 5 neighbors concluded with acc mean 0.5458461538461539 acc std 0.10457872660905562 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.5219703292999259 and std 0.18761463122057517.\n",
      "Test for 6 neighbors concluded with acc mean 0.541923076923077 acc std 0.10563784847101819 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.6871104092573647 and std 0.0849440801035858.\n",
      "Test for 7 neighbors concluded with acc mean 0.6150923076923077 acc std 0.07549432989406453 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.6468576754977282 and std 0.09935228614048093.\n",
      "Test for 8 neighbors concluded with acc mean 0.5980153846153846 acc std 0.07660744320906153 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.6838103158421005 and std 0.07649565748216088.\n",
      "Test for 9 neighbors concluded with acc mean 0.6142307692307692 acc std 0.07326708052568628 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.6503362989611345 and std 0.08977738073899282.\n",
      "Test for 10 neighbors concluded with acc mean 0.604876923076923 acc std 0.08110306249188208 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.6763050252910325 and std 0.07798072095355225.\n",
      "Test for 11 neighbors concluded with acc mean 0.6095076923076923 acc std 0.07475067196072477 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.6294160653663289 and std 0.11854475856176938.\n",
      "Test for 12 neighbors concluded with acc mean 0.5979230769230768 acc std 0.08911487832265462 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.6560247188648121 and std 0.09112955193571996.\n",
      "Test for 13 neighbors concluded with acc mean 0.6042923076923077 acc std 0.07776441284627207 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.635102246661685 and std 0.10330191255514405.\n",
      "Test for 14 neighbors concluded with acc mean 0.6087230769230769 acc std 0.0847554239768236 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_4.\n",
      "This test has f1 mean 0.6533706227513498 and std 0.09324816502566793.\n",
      "Test for 15 neighbors concluded with acc mean 0.6142923076923076 acc std 0.08126377388938791 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.5176535233449632 and std 0.14926829496255145.\n",
      "Test for 1 neighbors concluded with acc mean 0.5264 acc std 0.08840994348210429 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.347056489583613 and std 0.17283124648070544.\n",
      "Test for 2 neighbors concluded with acc mean 0.5100769230769231 acc std 0.07661673616355 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.4884875208854575 and std 0.14405113827352917.\n",
      "Test for 3 neighbors concluded with acc mean 0.5291230769230769 acc std 0.0783598904161785 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.41589348008640914 and std 0.16550731045856942.\n",
      "Test for 4 neighbors concluded with acc mean 0.5291846153846155 acc std 0.0791688944355916 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.519666380938851 and std 0.15682351722526805.\n",
      "Test for 5 neighbors concluded with acc mean 0.5447692307692308 acc std 0.08479407705116307 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.45754401253550875 and std 0.1593479945824136.\n",
      "Test for 6 neighbors concluded with acc mean 0.5319846153846154 acc std 0.07853459494738797 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.551530044132901 and std 0.13658336131871376.\n",
      "Test for 7 neighbors concluded with acc mean 0.5491230769230769 acc std 0.08316726000112469 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.5029413183685737 and std 0.15574395198477448.\n",
      "Test for 8 neighbors concluded with acc mean 0.533523076923077 acc std 0.08412143800102889 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.5786565148910413 and std 0.13464217627798314.\n",
      "Test for 9 neighbors concluded with acc mean 0.5534923076923077 acc std 0.08967656676757275 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.5488697989709549 and std 0.14351468410211954.\n",
      "Test for 10 neighbors concluded with acc mean 0.5432615384615385 acc std 0.0866598546860147 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.6067585491915327 and std 0.11597636561211895.\n",
      "Test for 11 neighbors concluded with acc mean 0.5689692307692308 acc std 0.08622306422723405 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.5639719993277985 and std 0.13716169131698733.\n",
      "Test for 12 neighbors concluded with acc mean 0.5476 acc std 0.08542876751731016 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.6285606915869727 and std 0.09983385577621179.\n",
      "Test for 13 neighbors concluded with acc mean 0.5805846153846153 acc std 0.07944952027664705 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.6122023160877732 and std 0.10625212948217441.\n",
      "Test for 14 neighbors concluded with acc mean 0.5754307692307692 acc std 0.08045858356097908 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features above_5.\n",
      "This test has f1 mean 0.6288776666976398 and std 0.10053907470359365.\n",
      "Test for 15 neighbors concluded with acc mean 0.5781846153846153 acc std 0.08069032922704697 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.6374600532702062 and std 0.12307040496270054.\n",
      "Test for 1 neighbors concluded with acc mean 0.6173692307692308 acc std 0.08872437074725885 and sureness of beating 73% 0.13.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.47371796592176585 and std 0.18830472580515237.\n",
      "Test for 2 neighbors concluded with acc mean 0.5689846153846154 acc std 0.09773983580382453 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.6192087832822242 and std 0.11586908274553011.\n",
      "Test for 3 neighbors concluded with acc mean 0.6094461538461539 acc std 0.08378191469982785 and sureness of beating 73% 0.1.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.5673346026619693 and std 0.13048488083088997.\n",
      "Test for 4 neighbors concluded with acc mean 0.6065538461538461 acc std 0.08619988330419483 and sureness of beating 73% 0.09.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.6258597619825865 and std 0.11599627631179486.\n",
      "Test for 5 neighbors concluded with acc mean 0.6091230769230769 acc std 0.09117205400753381 and sureness of beating 73% 0.09.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.568184229811821 and std 0.12271502025172443.\n",
      "Test for 6 neighbors concluded with acc mean 0.6047076923076923 acc std 0.0818596923447018 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.6192235312500592 and std 0.11180785233375512.\n",
      "Test for 7 neighbors concluded with acc mean 0.6075692307692306 acc std 0.09020389264616416 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.5794642229309047 and std 0.12404642204552359.\n",
      "Test for 8 neighbors concluded with acc mean 0.6098 acc std 0.08350761719853775 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.6301624781058252 and std 0.10352664173281743.\n",
      "Test for 9 neighbors concluded with acc mean 0.6119846153846153 acc std 0.07532911457702512 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.5977661234077932 and std 0.1165877730859693.\n",
      "Test for 10 neighbors concluded with acc mean 0.6067538461538462 acc std 0.07889200017940788 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.6541350956746064 and std 0.09381576443160046.\n",
      "Test for 11 neighbors concluded with acc mean 0.6158307692307692 acc std 0.0853748559911111 and sureness of beating 73% 0.09.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.6252982445853337 and std 0.10262814680432322.\n",
      "Test for 12 neighbors concluded with acc mean 0.6131846153846153 acc std 0.08101932124006707 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.6526603106471062 and std 0.0890042508795121.\n",
      "Test for 13 neighbors concluded with acc mean 0.6187230769230768 acc std 0.07907215040969988 and sureness of beating 73% 0.09.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.6158410091874692 and std 0.10701380716828984.\n",
      "Test for 14 neighbors concluded with acc mean 0.6099538461538462 acc std 0.08224352782938295 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X3.\n",
      "This test has f1 mean 0.6334545849224996 and std 0.1013959886214347.\n",
      "Test for 15 neighbors concluded with acc mean 0.6071076923076923 acc std 0.07706533921858606 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6012908454030298 and std 0.1444063872681363.\n",
      "Test for 1 neighbors concluded with acc mean 0.5670153846153845 acc std 0.1193767483530075 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.4541615042615105 and std 0.19634958526521354.\n",
      "Test for 2 neighbors concluded with acc mean 0.5526461538461539 acc std 0.10689342809740085 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6355496336514904 and std 0.12620926430071228.\n",
      "Test for 3 neighbors concluded with acc mean 0.6166615384615385 acc std 0.10489985192988235 and sureness of beating 73% 0.16.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.5132358971159197 and std 0.17889931395001052.\n",
      "Test for 4 neighbors concluded with acc mean 0.5677538461538462 acc std 0.11078865641631612 and sureness of beating 73% 0.1.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6592443633120065 and std 0.10549669248703357.\n",
      "Test for 5 neighbors concluded with acc mean 0.6223846153846153 acc std 0.09498361739776232 and sureness of beating 73% 0.14.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6116200706536903 and std 0.14751659116251822.\n",
      "Test for 6 neighbors concluded with acc mean 0.6189692307692307 acc std 0.10945827985851869 and sureness of beating 73% 0.19.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6633850696574687 and std 0.10218128135454436.\n",
      "Test for 7 neighbors concluded with acc mean 0.6302923076923076 acc std 0.09773958758992449 and sureness of beating 73% 0.19.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6494252568971071 and std 0.10636550458848215.\n",
      "Test for 8 neighbors concluded with acc mean 0.6394923076923075 acc std 0.0944795911088457 and sureness of beating 73% 0.21.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6537357394198027 and std 0.10007266791534354.\n",
      "Test for 9 neighbors concluded with acc mean 0.6256 acc std 0.08963265465813834 and sureness of beating 73% 0.12.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6397184139018494 and std 0.10001312639629514.\n",
      "Test for 10 neighbors concluded with acc mean 0.6358923076923076 acc std 0.08835031268467186 and sureness of beating 73% 0.14.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6429146173528616 and std 0.09684648654671557.\n",
      "Test for 11 neighbors concluded with acc mean 0.6209076923076923 acc std 0.08208441174113129 and sureness of beating 73% 0.1.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6379882951438808 and std 0.10115302526187187.\n",
      "Test for 12 neighbors concluded with acc mean 0.6319538461538462 acc std 0.08877359141402572 and sureness of beating 73% 0.17.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6428504096380999 and std 0.09374774311268266.\n",
      "Test for 13 neighbors concluded with acc mean 0.6192923076923077 acc std 0.09180695465528155 and sureness of beating 73% 0.12.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6401238870766265 and std 0.09826093882269023.\n",
      "Test for 14 neighbors concluded with acc mean 0.6359384615384616 acc std 0.09253830679698041 and sureness of beating 73% 0.16.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X5.\n",
      "This test has f1 mean 0.6405330769215292 and std 0.0955899613945102.\n",
      "Test for 15 neighbors concluded with acc mean 0.6212153846153846 acc std 0.09292315206313823 and sureness of beating 73% 0.11.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.6402043083278 and std 0.15411631218697433.\n",
      "Test for 1 neighbors concluded with acc mean 0.624476923076923 acc std 0.09499799935607489 and sureness of beating 73% 0.15.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.4322984456446837 and std 0.19552716424971875.\n",
      "Test for 2 neighbors concluded with acc mean 0.5642615384615385 acc std 0.08192791029439907 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.5702092310992329 and std 0.17981250082680258.\n",
      "Test for 3 neighbors concluded with acc mean 0.5994307692307692 acc std 0.09683992892562203 and sureness of beating 73% 0.09.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.4702199315954656 and std 0.2025538939687681.\n",
      "Test for 4 neighbors concluded with acc mean 0.5714153846153845 acc std 0.09185715177034909 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.6240376307724554 and std 0.1424997970532818.\n",
      "Test for 5 neighbors concluded with acc mean 0.6156461538461538 acc std 0.0975377469034728 and sureness of beating 73% 0.11.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.5349254581809949 and std 0.199055969983987.\n",
      "Test for 6 neighbors concluded with acc mean 0.5934615384615385 acc std 0.10387575076808415 and sureness of beating 73% 0.1.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.6233535941820729 and std 0.1438265243022167.\n",
      "Test for 7 neighbors concluded with acc mean 0.6145692307692308 acc std 0.09811054144787536 and sureness of beating 73% 0.13.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.5799133083181283 and std 0.1711895797059077.\n",
      "Test for 8 neighbors concluded with acc mean 0.6036923076923076 acc std 0.09705705026383792 and sureness of beating 73% 0.11.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.6370654454708612 and std 0.11747510415412091.\n",
      "Test for 9 neighbors concluded with acc mean 0.6093230769230769 acc std 0.09403456999970589 and sureness of beating 73% 0.09.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.6068049536431015 and std 0.13612818652436798.\n",
      "Test for 10 neighbors concluded with acc mean 0.6033384615384615 acc std 0.09479371686665973 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.6529951624592321 and std 0.09305378339172485.\n",
      "Test for 11 neighbors concluded with acc mean 0.6112923076923077 acc std 0.08343149973438686 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.6297714784312967 and std 0.11170040617485247.\n",
      "Test for 12 neighbors concluded with acc mean 0.6041230769230769 acc std 0.09083827361119719 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.6574368234709864 and std 0.08349249566646727.\n",
      "Test for 13 neighbors concluded with acc mean 0.6061384615384615 acc std 0.08248349162427926 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.6485608739041239 and std 0.092179422227725.\n",
      "Test for 14 neighbors concluded with acc mean 0.6124615384615384 acc std 0.08611551345027639 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,X6.\n",
      "This test has f1 mean 0.6670216150071124 and std 0.07896433678484095.\n",
      "Test for 15 neighbors concluded with acc mean 0.6112000000000001 acc std 0.08028590332174976 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.6274215707833443 and std 0.11746072130497895.\n",
      "Test for 1 neighbors concluded with acc mean 0.6121692307692307 acc std 0.09859080336864964 and sureness of beating 73% 0.1.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.49088454835879447 and std 0.13359218095942788.\n",
      "Test for 2 neighbors concluded with acc mean 0.5500769230769231 acc std 0.08427157423978784 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.6496304176849215 and std 0.0919199216062937.\n",
      "Test for 3 neighbors concluded with acc mean 0.6068615384615385 acc std 0.08895459116842049 and sureness of beating 73% 0.09.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.5621344827399736 and std 0.11781277251758362.\n",
      "Test for 4 neighbors concluded with acc mean 0.5833692307692309 acc std 0.09373099996815133 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.637445402500294 and std 0.0949835285004618.\n",
      "Test for 5 neighbors concluded with acc mean 0.5976615384615386 acc std 0.0904637571679772 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.5942357894716023 and std 0.11315263639899122.\n",
      "Test for 6 neighbors concluded with acc mean 0.5948307692307693 acc std 0.09155010139311019 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.6581463757037946 and std 0.08773487359651339.\n",
      "Test for 7 neighbors concluded with acc mean 0.6131076923076924 acc std 0.08673962397234557 and sureness of beating 73% 0.1.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.5937116895471106 and std 0.09541347412665835.\n",
      "Test for 8 neighbors concluded with acc mean 0.586446153846154 acc std 0.082937117407027 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.6282654537388193 and std 0.09878918889064145.\n",
      "Test for 9 neighbors concluded with acc mean 0.5815692307692307 acc std 0.08853441279254473 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.5817977704361378 and std 0.101986813726029.\n",
      "Test for 10 neighbors concluded with acc mean 0.5677230769230769 acc std 0.08237193179675342 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.6543260682629513 and std 0.08877704188820516.\n",
      "Test for 11 neighbors concluded with acc mean 0.5976307692307692 acc std 0.08601593907612663 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.6164262506287147 and std 0.09542778065123093.\n",
      "Test for 12 neighbors concluded with acc mean 0.5856615384615386 acc std 0.08492666659079968 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.6628075807121538 and std 0.08232893910393985.\n",
      "Test for 13 neighbors concluded with acc mean 0.5996153846153846 acc std 0.08921070729378838 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.632797140711953 and std 0.09512554031759424.\n",
      "Test for 14 neighbors concluded with acc mean 0.5936461538461538 acc std 0.08455344726140374 and sureness of beating 73% 0.1.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,mean.\n",
      "This test has f1 mean 0.6590113285473914 and std 0.08652774014415392.\n",
      "Test for 15 neighbors concluded with acc mean 0.5940769230769231 acc std 0.08679469757207073 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.6127560269542096 and std 0.09041447465751383.\n",
      "Test for 1 neighbors concluded with acc mean 0.5736 acc std 0.08700750698755454 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.4585527275462058 and std 0.1268675220665953.\n",
      "Test for 2 neighbors concluded with acc mean 0.5311076923076923 acc std 0.08025007659123704 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.6488653622260724 and std 0.09000443816632.\n",
      "Test for 3 neighbors concluded with acc mean 0.6065230769230769 acc std 0.09372302641546193 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.5553240378714516 and std 0.11985809616988172.\n",
      "Test for 4 neighbors concluded with acc mean 0.5700153846153847 acc std 0.09866889104054834 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.6397351278310727 and std 0.0978020053565375.\n",
      "Test for 5 neighbors concluded with acc mean 0.5941846153846153 acc std 0.10442892432983494 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.5737779940125916 and std 0.10753159802410618.\n",
      "Test for 6 neighbors concluded with acc mean 0.5774923076923077 acc std 0.09061255186900444 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.620964529407953 and std 0.09466354774235251.\n",
      "Test for 7 neighbors concluded with acc mean 0.5765384615384616 acc std 0.08832253292611805 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.5848250569116118 and std 0.11023635024079989.\n",
      "Test for 8 neighbors concluded with acc mean 0.5828923076923078 acc std 0.09254131461808074 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.6078653260621963 and std 0.09942688583478694.\n",
      "Test for 9 neighbors concluded with acc mean 0.5686461538461539 acc std 0.0897340660148101 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.5828150771407603 and std 0.09207726999926658.\n",
      "Test for 10 neighbors concluded with acc mean 0.5803384615384616 acc std 0.0757484407244753 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.6077867024335276 and std 0.09252153885831468.\n",
      "Test for 11 neighbors concluded with acc mean 0.573523076923077 acc std 0.07952917517579326 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.5885456038909721 and std 0.10428238631058469.\n",
      "Test for 12 neighbors concluded with acc mean 0.5881846153846154 acc std 0.08666001855854895 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.6140991597045541 and std 0.09618980421281932.\n",
      "Test for 13 neighbors concluded with acc mean 0.5804 acc std 0.08321061038397311 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.5890053221457474 and std 0.10228584061244723.\n",
      "Test for 14 neighbors concluded with acc mean 0.5870923076923077 acc std 0.08459605233700872 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,F_w_mean.\n",
      "This test has f1 mean 0.6220543528052802 and std 0.08770002527070644.\n",
      "Test for 15 neighbors concluded with acc mean 0.5883076923076922 acc std 0.0763520342629761 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.5670436308349082 and std 0.12980651063156637.\n",
      "Test for 1 neighbors concluded with acc mean 0.5532923076923076 acc std 0.10169068310118581 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.4200924065555045 and std 0.17892557785492985.\n",
      "Test for 2 neighbors concluded with acc mean 0.5218461538461537 acc std 0.0999490994717166 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.634740743983704 and std 0.09230315954751649.\n",
      "Test for 3 neighbors concluded with acc mean 0.5873076923076923 acc std 0.08272986624690487 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.5933432841024284 and std 0.12033658914667392.\n",
      "Test for 4 neighbors concluded with acc mean 0.5977076923076923 acc std 0.08476699564273739 and sureness of beating 73% 0.1.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.6473500470962449 and std 0.09247353326625295.\n",
      "Test for 5 neighbors concluded with acc mean 0.6042000000000001 acc std 0.08002539685632203 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.616944443549568 and std 0.10941190802061294.\n",
      "Test for 6 neighbors concluded with acc mean 0.6113692307692307 acc std 0.083623811053718 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.6570606233201971 and std 0.09391317688155078.\n",
      "Test for 7 neighbors concluded with acc mean 0.6102769230769229 acc std 0.0819553580314257 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.6072880904599379 and std 0.1116055156987127.\n",
      "Test for 8 neighbors concluded with acc mean 0.5949692307692306 acc std 0.08557106600526902 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.6722072821056928 and std 0.07877562101583617.\n",
      "Test for 9 neighbors concluded with acc mean 0.6206923076923077 acc std 0.07359938737846401 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.6440408723431134 and std 0.08468733876659476.\n",
      "Test for 10 neighbors concluded with acc mean 0.6185846153846153 acc std 0.07789499228118074 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.6725071572746526 and std 0.07163610950355742.\n",
      "Test for 11 neighbors concluded with acc mean 0.6167230769230769 acc std 0.06946240053984166 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.671722598252216 and std 0.08048865203056632.\n",
      "Test for 12 neighbors concluded with acc mean 0.6373692307692306 acc std 0.07823626635251113 and sureness of beating 73% 0.1.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.6873910380946424 and std 0.0736038360441298.\n",
      "Test for 13 neighbors concluded with acc mean 0.629 acc std 0.0751312461098033 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.6756152844099684 and std 0.07956103707561363.\n",
      "Test for 14 neighbors concluded with acc mean 0.6366153846153846 acc std 0.07640347586504397 and sureness of beating 73% 0.1.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_4.\n",
      "This test has f1 mean 0.6914317084494698 and std 0.07995694830871483.\n",
      "Test for 15 neighbors concluded with acc mean 0.6301076923076923 acc std 0.07529453173289372 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.5579936079341336 and std 0.1764774440227668.\n",
      "Test for 1 neighbors concluded with acc mean 0.5819384615384615 acc std 0.09224167382768238 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.40452795522107005 and std 0.16198871313277274.\n",
      "Test for 2 neighbors concluded with acc mean 0.5419384615384616 acc std 0.07429881138202056 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.5736354859372564 and std 0.13642094690623355.\n",
      "Test for 3 neighbors concluded with acc mean 0.5987538461538461 acc std 0.08696614181671627 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.5187174252264177 and std 0.13595667913059514.\n",
      "Test for 4 neighbors concluded with acc mean 0.5780461538461538 acc std 0.07831475107977034 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.6476196648492664 and std 0.10004636046371791.\n",
      "Test for 5 neighbors concluded with acc mean 0.6245846153846153 acc std 0.08408448273425002 and sureness of beating 73% 0.11.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.5870500215113096 and std 0.11472212058754212.\n",
      "Test for 6 neighbors concluded with acc mean 0.5996307692307693 acc std 0.0832172945156767 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.6376664279554722 and std 0.10741039322984267.\n",
      "Test for 7 neighbors concluded with acc mean 0.6143538461538461 acc std 0.08262929405898606 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.6241268629338139 and std 0.11342266350681338.\n",
      "Test for 8 neighbors concluded with acc mean 0.6183692307692307 acc std 0.08626416563202446 and sureness of beating 73% 0.09.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.6567476276893751 and std 0.09040643731665962.\n",
      "Test for 9 neighbors concluded with acc mean 0.6210923076923077 acc std 0.07672352423864295 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.6381588836317093 and std 0.1018709763179435.\n",
      "Test for 10 neighbors concluded with acc mean 0.6223692307692307 acc std 0.08165866901224922 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.6660992177102066 and std 0.08696909630680065.\n",
      "Test for 11 neighbors concluded with acc mean 0.6262307692307691 acc std 0.07533039651464246 and sureness of beating 73% 0.08.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.6558021157858295 and std 0.09400899522247592.\n",
      "Test for 12 neighbors concluded with acc mean 0.6302461538461538 acc std 0.07872751900397161 and sureness of beating 73% 0.1.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.6719946874443896 and std 0.084419040959972.\n",
      "Test for 13 neighbors concluded with acc mean 0.621476923076923 acc std 0.0732284808950806 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.6585200410244648 and std 0.09616885035448167.\n",
      "Test for 14 neighbors concluded with acc mean 0.6259692307692307 acc std 0.08059549960272606 and sureness of beating 73% 0.09.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X1,above_5.\n",
      "This test has f1 mean 0.676125076455589 and std 0.08699526820902805.\n",
      "Test for 15 neighbors concluded with acc mean 0.625076923076923 acc std 0.0760434319997188 and sureness of beating 73% 0.07.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.4102955252955403 and std 0.16325439176470294.\n",
      "Test for 1 neighbors concluded with acc mean 0.44503076923076923 acc std 0.09861546981875595 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.2820900344233684 and std 0.1996866553751391.\n",
      "Test for 2 neighbors concluded with acc mean 0.45924615384615386 acc std 0.09720557000623312 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.5091218436420887 and std 0.13414425703210267.\n",
      "Test for 3 neighbors concluded with acc mean 0.5050307692307691 acc std 0.097997187497379 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.42587914659730275 and std 0.17148243575010957.\n",
      "Test for 4 neighbors concluded with acc mean 0.5143692307692307 acc std 0.08983725456453079 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.5326542402982026 and std 0.1339051117792136.\n",
      "Test for 5 neighbors concluded with acc mean 0.5239692307692309 acc std 0.09934607016286419 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.46699625666477523 and std 0.16399780562972943.\n",
      "Test for 6 neighbors concluded with acc mean 0.5264 acc std 0.09728326722946878 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.5397687073154305 and std 0.13071470492587514.\n",
      "Test for 7 neighbors concluded with acc mean 0.5283692307692308 acc std 0.09584635634007503 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.48646227111838136 and std 0.15106268165730158.\n",
      "Test for 8 neighbors concluded with acc mean 0.5284615384615386 acc std 0.09116251175947193 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.5704902536024923 and std 0.10120909159388337.\n",
      "Test for 9 neighbors concluded with acc mean 0.5390615384615386 acc std 0.08222753681493135 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.5302012752094534 and std 0.11242999372693867.\n",
      "Test for 10 neighbors concluded with acc mean 0.5414461538461538 acc std 0.07949658769797169 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.5797388381048394 and std 0.1005464026007735.\n",
      "Test for 11 neighbors concluded with acc mean 0.5469538461538462 acc std 0.08370177548257213 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.5495518088128161 and std 0.1173149048969877.\n",
      "Test for 12 neighbors concluded with acc mean 0.5592153846153848 acc std 0.07968265311283625 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.5951873677922727 and std 0.09875440138923697.\n",
      "Test for 13 neighbors concluded with acc mean 0.5679692307692309 acc std 0.08312972111431856 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.5705025707812682 and std 0.10998491726213526.\n",
      "Test for 14 neighbors concluded with acc mean 0.5738461538461539 acc std 0.08846969862028749 and sureness of beating 73% 0.06.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X5.\n",
      "This test has f1 mean 0.5969438182976663 and std 0.0984266252140827.\n",
      "Test for 15 neighbors concluded with acc mean 0.5741538461538462 acc std 0.08126863919111929 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.5700758162143703 and std 0.12448011097805822.\n",
      "Test for 1 neighbors concluded with acc mean 0.5408923076923078 acc std 0.09205919885961726 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.43299823101044427 and std 0.1812418046373144.\n",
      "Test for 2 neighbors concluded with acc mean 0.5306615384615384 acc std 0.09996691878852226 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.5701671219700887 and std 0.11373313812357982.\n",
      "Test for 3 neighbors concluded with acc mean 0.527646153846154 acc std 0.08335124367689548 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.5083113433077362 and std 0.15236860488383308.\n",
      "Test for 4 neighbors concluded with acc mean 0.5522 acc std 0.09442300767661568 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.610471526412473 and std 0.10067408879335771.\n",
      "Test for 5 neighbors concluded with acc mean 0.5594 acc std 0.07675208533681868 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.5692359335452081 and std 0.10161556984238511.\n",
      "Test for 6 neighbors concluded with acc mean 0.5640923076923077 acc std 0.08529495500387128 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.6290591689000652 and std 0.08475256109765711.\n",
      "Test for 7 neighbors concluded with acc mean 0.5657384615384615 acc std 0.07589493048532808 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.5819163417855104 and std 0.10443448217287862.\n",
      "Test for 8 neighbors concluded with acc mean 0.5621384615384616 acc std 0.08532316430409682 and sureness of beating 73% 0.04.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.6272191025240845 and std 0.08109591372787928.\n",
      "Test for 9 neighbors concluded with acc mean 0.5570153846153847 acc std 0.07589086527344818 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.5900398419265548 and std 0.10928851706197715.\n",
      "Test for 10 neighbors concluded with acc mean 0.5553846153846155 acc std 0.08689407276481409 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.629726599574196 and std 0.079731680435596.\n",
      "Test for 11 neighbors concluded with acc mean 0.558646153846154 acc std 0.07367296357640417 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.6095211453166743 and std 0.08380423165070211.\n",
      "Test for 12 neighbors concluded with acc mean 0.569846153846154 acc std 0.07214285630591799 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.6291858526921326 and std 0.08141882472050851.\n",
      "Test for 13 neighbors concluded with acc mean 0.5637692307692308 acc std 0.07428025452983957 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.5962105069043563 and std 0.09969479446094218.\n",
      "Test for 14 neighbors concluded with acc mean 0.5644769230769231 acc std 0.08155155047339924 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,X6.\n",
      "This test has f1 mean 0.627306709901227 and std 0.07410338419243499.\n",
      "Test for 15 neighbors concluded with acc mean 0.5609538461538461 acc std 0.0720692566319321 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.5124147664965526 and std 0.11967882932787378.\n",
      "Test for 1 neighbors concluded with acc mean 0.47787692307692325 acc std 0.09760754166225638 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.3010520788890133 and std 0.1349395860926781.\n",
      "Test for 2 neighbors concluded with acc mean 0.4458615384615384 acc std 0.07153786434821044 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.5618339996386692 and std 0.11664786657807136.\n",
      "Test for 3 neighbors concluded with acc mean 0.5196307692307692 acc std 0.09189938413685768 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.44740614429335496 and std 0.14167579287329096.\n",
      "Test for 4 neighbors concluded with acc mean 0.49664615384615396 acc std 0.08696986623616305 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.6157283440562089 and std 0.09422806616433088.\n",
      "Test for 5 neighbors concluded with acc mean 0.5651692307692308 acc std 0.08839382558961267 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.5123357155904086 and std 0.10065245931809821.\n",
      "Test for 6 neighbors concluded with acc mean 0.5208923076923079 acc std 0.07758673100732087 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.6149526821130419 and std 0.0872089982007069.\n",
      "Test for 7 neighbors concluded with acc mean 0.5572307692307693 acc std 0.08469730300366521 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.5242160085524041 and std 0.10264117828992635.\n",
      "Test for 8 neighbors concluded with acc mean 0.5165076923076924 acc std 0.07934162670257429 and sureness of beating 73% 0.0.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.6198981983899882 and std 0.09245625008785358.\n",
      "Test for 9 neighbors concluded with acc mean 0.552323076923077 acc std 0.08608307395360469 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.56401395734141 and std 0.09768456535344873.\n",
      "Test for 10 neighbors concluded with acc mean 0.5289076923076924 acc std 0.0856726382618486 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.6348428832247834 and std 0.085152854179437.\n",
      "Test for 11 neighbors concluded with acc mean 0.5555076923076924 acc std 0.0894124622318772 and sureness of beating 73% 0.03.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.5883838035738672 and std 0.09345463673597458.\n",
      "Test for 12 neighbors concluded with acc mean 0.5443538461538463 acc std 0.0857879744955395 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.6358555171106858 and std 0.08627151210373278.\n",
      "Test for 13 neighbors concluded with acc mean 0.5590153846153847 acc std 0.09075189404188404 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.597548789712051 and std 0.09083267736952257.\n",
      "Test for 14 neighbors concluded with acc mean 0.5490615384615386 acc std 0.08071364974053563 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,mean.\n",
      "This test has f1 mean 0.6330008672754913 and std 0.08618324088434584.\n",
      "Test for 15 neighbors concluded with acc mean 0.5559076923076923 acc std 0.0807108125734848 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,F_w_mean.\n",
      "This test has f1 mean 0.5922875641068162 and std 0.08857034777680231.\n",
      "Test for 1 neighbors concluded with acc mean 0.5500461538461539 acc std 0.08370798919829159 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,F_w_mean.\n",
      "This test has f1 mean 0.4712711348548232 and std 0.12762501529934525.\n",
      "Test for 2 neighbors concluded with acc mean 0.5358923076923077 acc std 0.08544822440082905 and sureness of beating 73% 0.02.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,F_w_mean.\n",
      "This test has f1 mean 0.623500799566085 and std 0.08727196952199703.\n",
      "Test for 3 neighbors concluded with acc mean 0.5695384615384615 acc std 0.09176275043210019 and sureness of beating 73% 0.05.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,F_w_mean.\n",
      "This test has f1 mean 0.5240501681178368 and std 0.11215364238598417.\n",
      "Test for 4 neighbors concluded with acc mean 0.5373538461538462 acc std 0.08453620911117066 and sureness of beating 73% 0.01.\n",
      "____________________\n",
      "____________________\n",
      "Currently used features X3,F_w_mean.\n",
      "This test has f1 mean 0.628101745693577 and std 0.08682040813536442.\n",
      "Test for 5 neighbors concluded with acc mean 0.5653076923076923 acc std 0.08982201335329908 and sureness of beating 73% 0.03.\n",
      "____________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[140]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m y_te=np.ravel((y_te.values))\n\u001b[32m     31\u001b[39m NCA_KNN_pipe=Pipeline([(\u001b[33m\"\u001b[39m\u001b[33mDataCreate\u001b[39m\u001b[33m\"\u001b[39m, training.data_creator()),(\u001b[33m\"\u001b[39m\u001b[33mDataSelect\u001b[39m\u001b[33m\"\u001b[39m, training.data_selector(force=list_f_sel)),(\u001b[33m\"\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m\"\u001b[39m,StandardScaler()),(\u001b[33m\"\u001b[39m\u001b[33mKNN\u001b[39m\u001b[33m\"\u001b[39m,KNeighborsClassifier(n_neighbors=nn))])\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mNCA_KNN_pipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m y_p=NCA_KNN_pipe.predict(X=x_te)\n\u001b[32m     34\u001b[39m acc=accuracy_score(y_pred=y_p,y_true=y_te)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/sklearn/base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/sklearn/pipeline.py:653\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    647\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    648\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m     )\n\u001b[32m    652\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/sklearn/pipeline.py:587\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    581\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    582\u001b[39m     step_idx=step_idx,\n\u001b[32m    583\u001b[39m     step_params=routed_params[name],\n\u001b[32m    584\u001b[39m     all_params=raw_params,\n\u001b[32m    585\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/joblib/memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/sklearn/pipeline.py:1541\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1539\u001b[39m         res = transformer.fit_transform(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m, {}))\n\u001b[32m   1540\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res, transformer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/apziva/1_happy_customers/OAM6GH5slDAHadnQ/note_books/../proj_mod/training.py:63\u001b[39m, in \u001b[36mdata_creator.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# x_t_copy[\"mi_w_mean\"]=x_t_copy[features].apply(lambda row_arr: (np.array(row_arr)*mutual_info_classif(X=df2[features],y=df2[\"Y\"],discrete_features=True)).sum()/len(features), axis=1)\u001b[39;00m\n\u001b[32m     62\u001b[39m X_copy[\u001b[33m\"\u001b[39m\u001b[33mabove_3\u001b[39m\u001b[33m\"\u001b[39m]=X_copy[X.columns].apply(\u001b[38;5;28;01mlambda\u001b[39;00m row_arr: (row_arr >= \u001b[32m3\u001b[39m).mean(), axis=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m X_copy[\u001b[33m\"\u001b[39m\u001b[33mabove_4\u001b[39m\u001b[33m\"\u001b[39m]=\u001b[43mX_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_arr\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_arr\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m X_copy[\u001b[33m\"\u001b[39m\u001b[33mabove_5\u001b[39m\u001b[33m\"\u001b[39m]=X_copy[X.columns].apply(\u001b[38;5;28;01mlambda\u001b[39;00m row_arr: (row_arr >= \u001b[32m5\u001b[39m).mean(), axis=\u001b[32m1\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X_copy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/pandas/core/frame.py:10374\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10360\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10362\u001b[39m op = frame_apply(\n\u001b[32m  10363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10364\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10372\u001b[39m     kwargs=kwargs,\n\u001b[32m  10373\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/pandas/core/apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/pandas/core/apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/pandas/core/apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/apziva/1_happy_customers/OAM6GH5slDAHadnQ/note_books/../proj_mod/training.py:63\u001b[39m, in \u001b[36mdata_creator.transform.<locals>.<lambda>\u001b[39m\u001b[34m(row_arr)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# x_t_copy[\"mi_w_mean\"]=x_t_copy[features].apply(lambda row_arr: (np.array(row_arr)*mutual_info_classif(X=df2[features],y=df2[\"Y\"],discrete_features=True)).sum()/len(features), axis=1)\u001b[39;00m\n\u001b[32m     62\u001b[39m X_copy[\u001b[33m\"\u001b[39m\u001b[33mabove_3\u001b[39m\u001b[33m\"\u001b[39m]=X_copy[X.columns].apply(\u001b[38;5;28;01mlambda\u001b[39;00m row_arr: (row_arr >= \u001b[32m3\u001b[39m).mean(), axis=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m X_copy[\u001b[33m\"\u001b[39m\u001b[33mabove_4\u001b[39m\u001b[33m\"\u001b[39m]=X_copy[X.columns].apply(\u001b[38;5;28;01mlambda\u001b[39;00m row_arr: (\u001b[43mrow_arr\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m).mean(), axis=\u001b[32m1\u001b[39m)\n\u001b[32m     64\u001b[39m X_copy[\u001b[33m\"\u001b[39m\u001b[33mabove_5\u001b[39m\u001b[33m\"\u001b[39m]=X_copy[X.columns].apply(\u001b[38;5;28;01mlambda\u001b[39;00m row_arr: (row_arr >= \u001b[32m5\u001b[39m).mean(), axis=\u001b[32m1\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X_copy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/pandas/core/arraylike.py:60\u001b[39m, in \u001b[36mOpsMixin.__ge__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__ge__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/pandas/core/series.py:6121\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6117\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   6119\u001b[39m res_values = ops.comparison_op(lvalues, rvalues, op)\n\u001b[32m-> \u001b[39m\u001b[32m6121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_construct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mres_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/pandas/core/series.py:6231\u001b[39m, in \u001b[36mSeries._construct_result\u001b[39m\u001b[34m(self, result, name)\u001b[39m\n\u001b[32m   6228\u001b[39m \u001b[38;5;66;03m# TODO: result should always be ArrayLike, but this fails for some\u001b[39;00m\n\u001b[32m   6229\u001b[39m \u001b[38;5;66;03m#  JSONArray tests\u001b[39;00m\n\u001b[32m   6230\u001b[39m dtype = \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6231\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   6232\u001b[39m out = out.__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   6234\u001b[39m \u001b[38;5;66;03m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[32m   6235\u001b[39m \u001b[38;5;66;03m#  would set it back to self.name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/pandas/core/series.py:389\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    384\u001b[39m _mgr: SingleManager\n\u001b[32m    386\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[38;5;66;03m# Constructors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    391\u001b[39m     data=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    392\u001b[39m     index=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    393\u001b[39m     dtype: Dtype | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    394\u001b[39m     name=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    395\u001b[39m     copy: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    396\u001b[39m     fastpath: \u001b[38;5;28mbool\u001b[39m | lib.NoDefault = lib.no_default,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fastpath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    399\u001b[39m         warnings.warn(\n\u001b[32m    400\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastpath\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword in pd.Series is deprecated and will \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbe removed in a future version.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    402\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    403\u001b[39m             stacklevel=find_stack_level(),\n\u001b[32m    404\u001b[39m         )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# range_nc=range(1,6+1)\n",
    "range_nn=range(1,15+1)\n",
    "range_feat_combin=training.all_combin(eva_out.columns)\n",
    "print(len(range_feat_combin))\n",
    "\n",
    "n_split=5\n",
    "n_repeats=20\n",
    "\n",
    "# list_nc=[]\n",
    "list_nn=[]\n",
    "list_feat=[]\n",
    "list_acc_mean=[]\n",
    "list_acc_std=[]\n",
    "list_above_73=[]\n",
    "list_f1_mean=[]\n",
    "list_f1_std=[]\n",
    "\n",
    "RSKF=RepeatedStratifiedKFold(n_splits=n_split,random_state=420,n_repeats=n_repeats)\n",
    "\n",
    "for list_f_sel,nn in itertools.product(range_feat_combin,range_nn): \n",
    "    list_f_sel=list(list_f_sel)\n",
    "    list_fold_acc=[]\n",
    "    list_fold_f1=[]\n",
    "    for train_index, test_index in RSKF.split(X=feat,y=tar): \n",
    "        x_tr,x_te=feat.iloc[train_index], feat.iloc[test_index]\n",
    "        y_tr, y_te=tar.iloc[train_index], tar.iloc[test_index]\n",
    "        \n",
    "        y_tr=np.ravel(y_tr.values)\n",
    "        y_te=np.ravel((y_te.values))\n",
    "        \n",
    "        NCA_KNN_pipe=Pipeline([(\"DataCreate\", training.data_creator()),(\"DataSelect\", training.data_selector(force=list_f_sel)),(\"scale\",StandardScaler()),(\"KNN\",KNeighborsClassifier(n_neighbors=nn))])\n",
    "        NCA_KNN_pipe.fit(X=x_tr,y=y_tr)\n",
    "        y_p=NCA_KNN_pipe.predict(X=x_te)\n",
    "        acc=accuracy_score(y_pred=y_p,y_true=y_te)\n",
    "        f1=f1_score(y_pred=y_p,y_true=y_te)\n",
    "        list_fold_acc.append(acc)\n",
    "        list_fold_f1.append(f1)\n",
    "    str_features=\",\".join(list_f_sel)\n",
    "    list_feat.append(str_features)\n",
    "    list_nn.append(nn)\n",
    "    list_acc_mean.append(np.mean(list_fold_acc))\n",
    "    list_acc_std.append(np.std(list_fold_acc))\n",
    "    list_f1_mean.append(np.mean(list_fold_f1))\n",
    "    list_f1_std.append(np.std(list_fold_f1))\n",
    "    list_above_73.append((np.array(list_fold_acc)>=0.73).sum()/(n_split*n_repeats))\n",
    "    print(\"_\"*20+\"\\n\"+f\"Currently used features {str_features}.\")\n",
    "    print(f\"This test has f1 mean {np.mean(list_fold_f1)} and std {np.std(list_fold_f1)}.\")\n",
    "    print(f\"Test for {nn} neighbors concluded with acc mean {np.mean(list_fold_acc)} acc std {np.std(list_fold_acc)} and sureness of beating 73% {(np.array(list_fold_acc)>=0.73).sum()/(n_split*n_repeats)}.\"+\"\\n\"+\"_\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8cc189",
   "metadata": {},
   "source": [
    "I cut above early, it is taking too long, let's look at the result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "90ff4809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>nn</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>above_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>X1,X5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.639492</td>\n",
       "      <td>0.094480</td>\n",
       "      <td>0.649425</td>\n",
       "      <td>0.106366</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>X1,X5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.630292</td>\n",
       "      <td>0.097740</td>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.102181</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>X1,X5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.618969</td>\n",
       "      <td>0.109458</td>\n",
       "      <td>0.611620</td>\n",
       "      <td>0.147517</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>X1,X5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.631954</td>\n",
       "      <td>0.088774</td>\n",
       "      <td>0.637988</td>\n",
       "      <td>0.101153</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>X1,X5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.635938</td>\n",
       "      <td>0.092538</td>\n",
       "      <td>0.640124</td>\n",
       "      <td>0.098261</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>X3,X6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.559400</td>\n",
       "      <td>0.076752</td>\n",
       "      <td>0.610472</td>\n",
       "      <td>0.100674</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>X3,X6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.558646</td>\n",
       "      <td>0.073673</td>\n",
       "      <td>0.629727</td>\n",
       "      <td>0.079732</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>X3,X6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.565738</td>\n",
       "      <td>0.075895</td>\n",
       "      <td>0.629059</td>\n",
       "      <td>0.084753</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>X3,X6</td>\n",
       "      <td>12</td>\n",
       "      <td>0.569846</td>\n",
       "      <td>0.072143</td>\n",
       "      <td>0.609521</td>\n",
       "      <td>0.083804</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>X3,mean</td>\n",
       "      <td>1</td>\n",
       "      <td>0.477877</td>\n",
       "      <td>0.097608</td>\n",
       "      <td>0.512415</td>\n",
       "      <td>0.119679</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    features  nn  acc_mean   acc_std   f1_mean    f1_std  above_73\n",
       "142    X1,X5   8  0.639492  0.094480  0.649425  0.106366      0.21\n",
       "141    X1,X5   7  0.630292  0.097740  0.663385  0.102181      0.19\n",
       "140    X1,X5   6  0.618969  0.109458  0.611620  0.147517      0.19\n",
       "146    X1,X5  12  0.631954  0.088774  0.637988  0.101153      0.17\n",
       "148    X1,X5  14  0.635938  0.092538  0.640124  0.098261      0.16\n",
       "..       ...  ..       ...       ...       ...       ...       ...\n",
       "244    X3,X6   5  0.559400  0.076752  0.610472  0.100674      0.00\n",
       "250    X3,X6  11  0.558646  0.073673  0.629727  0.079732      0.00\n",
       "246    X3,X6   7  0.565738  0.075895  0.629059  0.084753      0.00\n",
       "251    X3,X6  12  0.569846  0.072143  0.609521  0.083804      0.00\n",
       "255  X3,mean   1  0.477877  0.097608  0.512415  0.119679      0.00\n",
       "\n",
       "[275 rows x 7 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame({\n",
    "    \"features\": list_feat,\n",
    "    \"nn\": list_nn,\n",
    "    \"acc_mean\": list_acc_mean,\n",
    "    \"acc_std\": list_acc_std,\n",
    "    \"f1_mean\": list_f1_mean,\n",
    "    \"f1_std\": list_f1_std,\n",
    "    \"above_73\": list_above_73,\n",
    "})\n",
    "\n",
    "df_results.sort_values(by=\"above_73\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917d117",
   "metadata": {},
   "source": [
    "Even with only X1 and X5, we can manage to make a pretty good model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9b71737e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_fold_acc=[]\n",
    "list_fold_f1=[]\n",
    "for train_index, test_index in RSKF.split(X=feat,y=tar): \n",
    "    x_tr,x_te=feat.iloc[train_index], feat.iloc[test_index]\n",
    "    y_tr, y_te=tar.iloc[train_index], tar.iloc[test_index]\n",
    "        \n",
    "    y_tr=np.ravel(y_tr.values)\n",
    "    y_te=np.ravel((y_te.values))\n",
    "        \n",
    "    NCA_KNN_pipe=Pipeline([(\"DataCreate\", training.data_creator()),(\"DataSelect\", training.data_selector(force=[\"X1\",\"X5\"])),(\"scale\",StandardScaler()),(\"KNN\",KNeighborsClassifier(n_neighbors=8))])\n",
    "    NCA_KNN_pipe.fit(X=x_tr,y=y_tr)\n",
    "    y_p=NCA_KNN_pipe.predict(X=x_te)\n",
    "    acc=accuracy_score(y_pred=y_p,y_true=y_te)\n",
    "    f1=f1_score(y_pred=y_p,y_true=y_te)\n",
    "    list_fold_acc.append(acc)\n",
    "    list_fold_f1.append(f1)\n",
    "    \n",
    "plt.hist(list_fold_acc,bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc62b7",
   "metadata": {},
   "source": [
    "I have 21% sureness that above model can beat the 73% line. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdc921",
   "metadata": {},
   "source": [
    "I will rewrite the code so that it can be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a674ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "# os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "# os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c3e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:   25.1s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   32.2s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   39.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   50.0s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1426 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1481 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1536 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1593 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1650 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1709 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1829 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1890 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1953 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2016 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2081 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2146 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2213 tasks      | elapsed: 18.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2280 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2349 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed: 20.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2489 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2560 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2633 tasks      | elapsed: 21.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2706 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2781 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2856 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2933 tasks      | elapsed: 24.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3010 tasks      | elapsed: 24.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3089 tasks      | elapsed: 25.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed: 26.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3249 tasks      | elapsed: 26.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3330 tasks      | elapsed: 27.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3413 tasks      | elapsed: 28.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3496 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3581 tasks      | elapsed: 29.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3666 tasks      | elapsed: 30.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3753 tasks      | elapsed: 31.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3825 out of 3825 | elapsed: 31.6min finished\n"
     ]
    }
   ],
   "source": [
    "range_nn = range(1, 15 + 1)\n",
    "range_feat_combin = training.all_combin(eva_out.columns)\n",
    "print(len(range_feat_combin))\n",
    "\n",
    "n_split = 5\n",
    "n_repeats = 20\n",
    "\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=n_split, random_state=420, n_repeats=n_repeats)\n",
    "\n",
    "splits = list(RSKF.split(X=feat, y=tar))\n",
    "\n",
    "def evaluate_combo(list_f_sel_tuple, nn, splits, feat, tar):\n",
    "    \"\"\"\n",
    "    Evaluate one (feature_set, n_neighbors) across all CV folds.\n",
    "    \n",
    "    :param list_f_sel_tuple: A tuple indicating a combination. \n",
    "    :param nn: n_neighbors \n",
    "    :param splits: A list of the pre generated splits. \n",
    "    :param feat: The feat df. \n",
    "    :param tar: The tar df. \n",
    "    :return: A dict with all the stats we want. \n",
    "    \"\"\"\n",
    "    list_f_sel = list(list_f_sel_tuple) \n",
    "    fold_acc = []\n",
    "    fold_f1 = []\n",
    "\n",
    "    for train_index, test_index in splits:\n",
    "        x_tr, x_te = feat.iloc[train_index], feat.iloc[test_index]\n",
    "        y_tr, y_te = tar.iloc[train_index], tar.iloc[test_index]\n",
    "\n",
    "        y_tr = np.ravel(y_tr.values)\n",
    "        y_te = np.ravel(y_te.values)\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"DataCreate\", training.data_creator()),\n",
    "            (\"DataSelect\", training.data_selector(force=list_f_sel)),\n",
    "            (\"scale\", StandardScaler()),\n",
    "            (\"KNN\", KNeighborsClassifier(n_neighbors=nn)),\n",
    "        ])\n",
    "\n",
    "        pipe.fit(X=x_tr, y=y_tr)\n",
    "        y_p = pipe.predict(X=x_te)\n",
    "\n",
    "        fold_acc.append(accuracy_score(y_true=y_te, y_pred=y_p))\n",
    "        fold_f1.append(f1_score(y_true=y_te, y_pred=y_p))\n",
    "\n",
    "    str_features = \",\".join(list_f_sel)\n",
    "    acc_mean = float(np.mean(fold_acc))\n",
    "    acc_std  = float(np.std(fold_acc))\n",
    "    f1_mean  = float(np.mean(fold_f1))\n",
    "    f1_std   = float(np.std(fold_f1))\n",
    "    above_73 = float((np.array(fold_acc) >= 0.73).sum() / (len(splits)))\n",
    "\n",
    "    msg = (\n",
    "        \"_\"*20 + \"\\n\"\n",
    "        + f\"Currently used features {str_features} and {nn} neighbors.\\n\"\n",
    "        + f\"This combo has f1 mean {f1_mean} and f1 std {f1_std}, \\n\"\n",
    "        + f\"with acc mean {acc_mean} acc std {acc_std}, \"\n",
    "        + f\"and sureness of beating 73% {above_73}.\\n\"\n",
    "        + \"_\"*20\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"features\": str_features,\n",
    "        \"nn\": nn,\n",
    "        \"acc_mean\": acc_mean,\n",
    "        \"acc_std\": acc_std,\n",
    "        \"f1_mean\": f1_mean,\n",
    "        \"f1_std\": f1_std,\n",
    "        \"above_73\": above_73,\n",
    "        \"log\": msg,\n",
    "    }\n",
    "\n",
    "jobs = list(itertools.product(range_feat_combin, range_nn))\n",
    "\n",
    "results = Parallel(n_jobs=-1, backend=\"loky\", verbose=10)(\n",
    "    delayed(evaluate_combo)(feat_sel, nn, splits, feat, tar)\n",
    "    for feat_sel, nn in jobs\n",
    ")\n",
    "\n",
    "list_feat      = [r[\"features\"] for r in results]\n",
    "list_nn        = [r[\"nn\"] for r in results]\n",
    "list_acc_mean  = [r[\"acc_mean\"] for r in results]\n",
    "list_acc_std   = [r[\"acc_std\"] for r in results]\n",
    "list_f1_mean   = [r[\"f1_mean\"] for r in results]\n",
    "list_f1_std    = [r[\"f1_std\"] for r in results]\n",
    "list_above_73  = [r[\"above_73\"] for r in results]\n",
    "\n",
    "# for r in results: #Too much info to read \n",
    "#     print(r[\"log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f5fc95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({\n",
    "    \"features\": list_feat,\n",
    "    \"nn\": list_nn,\n",
    "    \"acc_mean\": list_acc_mean,\n",
    "    \"acc_std\": list_acc_std,\n",
    "    \"f1_mean\": list_f1_mean,\n",
    "    \"f1_std\": list_f1_std,\n",
    "    \"above_73\": list_above_73,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d6fac2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>nn</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>above_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>X1,X3,X6,above_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702754</td>\n",
       "      <td>0.092395</td>\n",
       "      <td>0.739808</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>X1,X6,F_w_mean,above_4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.704123</td>\n",
       "      <td>0.083316</td>\n",
       "      <td>0.736048</td>\n",
       "      <td>0.079095</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>7</td>\n",
       "      <td>0.689662</td>\n",
       "      <td>0.097942</td>\n",
       "      <td>0.732151</td>\n",
       "      <td>0.088189</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>X1,X3,X6,F_w_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.695415</td>\n",
       "      <td>0.088707</td>\n",
       "      <td>0.725816</td>\n",
       "      <td>0.082690</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.705277</td>\n",
       "      <td>0.087925</td>\n",
       "      <td>0.746999</td>\n",
       "      <td>0.076984</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>X6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.086350</td>\n",
       "      <td>0.332990</td>\n",
       "      <td>0.300394</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>X1,mean,above_4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500785</td>\n",
       "      <td>0.089333</td>\n",
       "      <td>0.431312</td>\n",
       "      <td>0.139063</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>X6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.488062</td>\n",
       "      <td>0.069151</td>\n",
       "      <td>0.218731</td>\n",
       "      <td>0.276586</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>X3,mean,above_4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.495215</td>\n",
       "      <td>0.083525</td>\n",
       "      <td>0.407389</td>\n",
       "      <td>0.126053</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>X5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.539015</td>\n",
       "      <td>0.091366</td>\n",
       "      <td>0.444601</td>\n",
       "      <td>0.137679</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3825 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    features  nn  acc_mean   acc_std   f1_mean    f1_std  \\\n",
       "1485        X1,X3,X6,above_4   1  0.702754  0.092395  0.739808  0.084133   \n",
       "1802  X1,X6,F_w_mean,above_4   3  0.704123  0.083316  0.736048  0.079095   \n",
       "726           X1,X6,F_w_mean   7  0.689662  0.097942  0.732151  0.088189   \n",
       "1474       X1,X3,X6,F_w_mean   5  0.695415  0.088707  0.725816  0.082690   \n",
       "724           X1,X6,F_w_mean   5  0.705277  0.087925  0.746999  0.076984   \n",
       "...                      ...  ..       ...       ...       ...       ...   \n",
       "48                        X6   4  0.516800  0.086350  0.332990  0.300394   \n",
       "781          X1,mean,above_4   2  0.500785  0.089333  0.431312  0.139063   \n",
       "46                        X6   2  0.488062  0.069151  0.218731  0.276586   \n",
       "1006         X3,mean,above_4   2  0.495215  0.083525  0.407389  0.126053   \n",
       "43                        X5  14  0.539015  0.091366  0.444601  0.137679   \n",
       "\n",
       "      above_73  \n",
       "1485      0.41  \n",
       "1802      0.40  \n",
       "726       0.39  \n",
       "1474      0.38  \n",
       "724       0.37  \n",
       "...        ...  \n",
       "48        0.00  \n",
       "781       0.00  \n",
       "46        0.00  \n",
       "1006      0.00  \n",
       "43        0.00  \n",
       "\n",
       "[3825 rows x 7 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=[\"above_73\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df0c96",
   "metadata": {},
   "source": [
    "As a clarification, below I add in a norm column called \"norm_above_73\", it suggests the confidence level of the model beating 73% assuming the distribution is norm with the same mean and std as current distribution. \n",
    "\n",
    "While \"acc_mean_above_73\" suggests the confidence level that the \"acc_mean\" can beat 73%. This is based on CLT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c0bd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>nn</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>above_73</th>\n",
       "      <th>norm_above_73</th>\n",
       "      <th>acc_mean_above_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>X1,X3,X6,above_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702754</td>\n",
       "      <td>0.092395</td>\n",
       "      <td>0.739808</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.384039</td>\n",
       "      <td>0.001595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>X1,X6,F_w_mean,above_4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.704123</td>\n",
       "      <td>0.083316</td>\n",
       "      <td>0.736048</td>\n",
       "      <td>0.079095</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.000949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>7</td>\n",
       "      <td>0.689662</td>\n",
       "      <td>0.097942</td>\n",
       "      <td>0.732151</td>\n",
       "      <td>0.088189</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.340221</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>X1,X3,X6,F_w_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.695415</td>\n",
       "      <td>0.088707</td>\n",
       "      <td>0.725816</td>\n",
       "      <td>0.082690</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.705277</td>\n",
       "      <td>0.087925</td>\n",
       "      <td>0.746999</td>\n",
       "      <td>0.076984</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.389284</td>\n",
       "      <td>0.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>X6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.086350</td>\n",
       "      <td>0.332990</td>\n",
       "      <td>0.300394</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>X1,mean,above_4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500785</td>\n",
       "      <td>0.089333</td>\n",
       "      <td>0.431312</td>\n",
       "      <td>0.139063</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005146</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>X6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.488062</td>\n",
       "      <td>0.069151</td>\n",
       "      <td>0.218731</td>\n",
       "      <td>0.276586</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>X3,mean,above_4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.495215</td>\n",
       "      <td>0.083525</td>\n",
       "      <td>0.407389</td>\n",
       "      <td>0.126053</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>X5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.539015</td>\n",
       "      <td>0.091366</td>\n",
       "      <td>0.444601</td>\n",
       "      <td>0.137679</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.018294</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3825 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    features  nn  acc_mean   acc_std   f1_mean    f1_std  \\\n",
       "1485        X1,X3,X6,above_4   1  0.702754  0.092395  0.739808  0.084133   \n",
       "1802  X1,X6,F_w_mean,above_4   3  0.704123  0.083316  0.736048  0.079095   \n",
       "726           X1,X6,F_w_mean   7  0.689662  0.097942  0.732151  0.088189   \n",
       "1474       X1,X3,X6,F_w_mean   5  0.695415  0.088707  0.725816  0.082690   \n",
       "724           X1,X6,F_w_mean   5  0.705277  0.087925  0.746999  0.076984   \n",
       "...                      ...  ..       ...       ...       ...       ...   \n",
       "48                        X6   4  0.516800  0.086350  0.332990  0.300394   \n",
       "781          X1,mean,above_4   2  0.500785  0.089333  0.431312  0.139063   \n",
       "46                        X6   2  0.488062  0.069151  0.218731  0.276586   \n",
       "1006         X3,mean,above_4   2  0.495215  0.083525  0.407389  0.126053   \n",
       "43                        X5  14  0.539015  0.091366  0.444601  0.137679   \n",
       "\n",
       "      above_73  norm_above_73  acc_mean_above_73  \n",
       "1485      0.41       0.384039           0.001595  \n",
       "1802      0.40       0.378057           0.000949  \n",
       "726       0.39       0.340221           0.000019  \n",
       "1474      0.38       0.348315           0.000048  \n",
       "724       0.37       0.389284           0.002463  \n",
       "...        ...            ...                ...  \n",
       "48        0.00       0.006774           0.000000  \n",
       "781       0.00       0.005146           0.000000  \n",
       "46        0.00       0.000234           0.000000  \n",
       "1006      0.00       0.002470           0.000000  \n",
       "43        0.00       0.018294           0.000000  \n",
       "\n",
       "[3825 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[\"norm_above_73\"]= df_results.apply(lambda row: (1- norm.cdf(0.73,loc=row[\"acc_mean\"],scale=row[\"acc_std\"])),axis=1)\n",
    "df_results[\"acc_mean_above_73\"]= df_results.apply(lambda row: (1- norm.cdf(0.73,loc=row[\"acc_mean\"],scale=row[\"acc_std\"]/10)),axis=1)\n",
    "df_results.sort_values(by=[\"above_73\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f91ddde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>nn</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>above_73</th>\n",
       "      <th>norm_above_73</th>\n",
       "      <th>acc_mean_above_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.705277</td>\n",
       "      <td>0.087925</td>\n",
       "      <td>0.746999</td>\n",
       "      <td>0.076984</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.389284</td>\n",
       "      <td>0.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>X1,X3,X6,above_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702754</td>\n",
       "      <td>0.092395</td>\n",
       "      <td>0.739808</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.384039</td>\n",
       "      <td>0.001595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>X1,X6,F_w_mean,above_4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.704123</td>\n",
       "      <td>0.083316</td>\n",
       "      <td>0.736048</td>\n",
       "      <td>0.079095</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.000949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>X1,X3,X6,F_w_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.695415</td>\n",
       "      <td>0.088707</td>\n",
       "      <td>0.725816</td>\n",
       "      <td>0.082690</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>3</td>\n",
       "      <td>0.694277</td>\n",
       "      <td>0.089131</td>\n",
       "      <td>0.730765</td>\n",
       "      <td>0.079619</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.344286</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>X6,F_w_mean,above_4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.638338</td>\n",
       "      <td>0.091708</td>\n",
       "      <td>0.695275</td>\n",
       "      <td>0.080358</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.158779</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>X6,F_w_mean,above_4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.639846</td>\n",
       "      <td>0.101402</td>\n",
       "      <td>0.680891</td>\n",
       "      <td>0.096248</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.186982</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>X6,F_w_mean,above_4</td>\n",
       "      <td>13</td>\n",
       "      <td>0.628492</td>\n",
       "      <td>0.086324</td>\n",
       "      <td>0.692535</td>\n",
       "      <td>0.075953</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.119819</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>X6,F_w_mean,above_4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.636723</td>\n",
       "      <td>0.083556</td>\n",
       "      <td>0.686722</td>\n",
       "      <td>0.077777</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.132138</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>X6,mean,above_5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.596662</td>\n",
       "      <td>0.086063</td>\n",
       "      <td>0.610765</td>\n",
       "      <td>0.092432</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.060654</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3825 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    features  nn  acc_mean   acc_std   f1_mean    f1_std  \\\n",
       "724           X1,X6,F_w_mean   5  0.705277  0.087925  0.746999  0.076984   \n",
       "1485        X1,X3,X6,above_4   1  0.702754  0.092395  0.739808  0.084133   \n",
       "1802  X1,X6,F_w_mean,above_4   3  0.704123  0.083316  0.736048  0.079095   \n",
       "1474       X1,X3,X6,F_w_mean   5  0.695415  0.088707  0.725816  0.082690   \n",
       "722           X1,X6,F_w_mean   3  0.694277  0.089131  0.730765  0.079619   \n",
       "...                      ...  ..       ...       ...       ...       ...   \n",
       "1285     X6,F_w_mean,above_4  11  0.638338  0.091708  0.695275  0.080358   \n",
       "1286     X6,F_w_mean,above_4  12  0.639846  0.101402  0.680891  0.096248   \n",
       "1287     X6,F_w_mean,above_4  13  0.628492  0.086324  0.692535  0.075953   \n",
       "1288     X6,F_w_mean,above_4  14  0.636723  0.083556  0.686722  0.077777   \n",
       "1269         X6,mean,above_5  10  0.596662  0.086063  0.610765  0.092432   \n",
       "\n",
       "      above_73  norm_above_73  acc_mean_above_73  \n",
       "724       0.37       0.389284           0.002463  \n",
       "1485      0.41       0.384039           0.001595  \n",
       "1802      0.40       0.378057           0.000949  \n",
       "1474      0.38       0.348315           0.000048  \n",
       "722       0.37       0.344286           0.000031  \n",
       "...        ...            ...                ...  \n",
       "1285      0.16       0.158779           0.000000  \n",
       "1286      0.18       0.186982           0.000000  \n",
       "1287      0.10       0.119819           0.000000  \n",
       "1288      0.12       0.132138           0.000000  \n",
       "1269      0.08       0.060654           0.000000  \n",
       "\n",
       "[3825 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=[\"acc_mean_above_73\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d564ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>nn</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>above_73</th>\n",
       "      <th>norm_above_73</th>\n",
       "      <th>acc_mean_above_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.705277</td>\n",
       "      <td>0.087925</td>\n",
       "      <td>0.746999</td>\n",
       "      <td>0.076984</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.389284</td>\n",
       "      <td>0.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>X1,X3,X6,above_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702754</td>\n",
       "      <td>0.092395</td>\n",
       "      <td>0.739808</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.384039</td>\n",
       "      <td>0.001595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>X1,X6,F_w_mean,above_4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.704123</td>\n",
       "      <td>0.083316</td>\n",
       "      <td>0.736048</td>\n",
       "      <td>0.079095</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.000949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>X1,X3,X6,F_w_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.695415</td>\n",
       "      <td>0.088707</td>\n",
       "      <td>0.725816</td>\n",
       "      <td>0.082690</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.348315</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>3</td>\n",
       "      <td>0.694277</td>\n",
       "      <td>0.089131</td>\n",
       "      <td>0.730765</td>\n",
       "      <td>0.079619</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.344286</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>X5,mean</td>\n",
       "      <td>2</td>\n",
       "      <td>0.487077</td>\n",
       "      <td>0.070249</td>\n",
       "      <td>0.362040</td>\n",
       "      <td>0.125661</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>X6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.488062</td>\n",
       "      <td>0.069151</td>\n",
       "      <td>0.218731</td>\n",
       "      <td>0.276586</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>X3,X5,mean</td>\n",
       "      <td>2</td>\n",
       "      <td>0.449585</td>\n",
       "      <td>0.078070</td>\n",
       "      <td>0.326494</td>\n",
       "      <td>0.125134</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>X3,mean</td>\n",
       "      <td>2</td>\n",
       "      <td>0.445862</td>\n",
       "      <td>0.071538</td>\n",
       "      <td>0.301052</td>\n",
       "      <td>0.134940</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>X3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.484523</td>\n",
       "      <td>0.059224</td>\n",
       "      <td>0.181147</td>\n",
       "      <td>0.208995</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3825 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    features  nn  acc_mean   acc_std   f1_mean    f1_std  \\\n",
       "724           X1,X6,F_w_mean   5  0.705277  0.087925  0.746999  0.076984   \n",
       "1485        X1,X3,X6,above_4   1  0.702754  0.092395  0.739808  0.084133   \n",
       "1802  X1,X6,F_w_mean,above_4   3  0.704123  0.083316  0.736048  0.079095   \n",
       "1474       X1,X3,X6,F_w_mean   5  0.695415  0.088707  0.725816  0.082690   \n",
       "722           X1,X6,F_w_mean   3  0.694277  0.089131  0.730765  0.079619   \n",
       "...                      ...  ..       ...       ...       ...       ...   \n",
       "331                  X5,mean   2  0.487077  0.070249  0.362040  0.125661   \n",
       "46                        X6   2  0.488062  0.069151  0.218731  0.276586   \n",
       "871               X3,X5,mean   2  0.449585  0.078070  0.326494  0.125134   \n",
       "256                  X3,mean   2  0.445862  0.071538  0.301052  0.134940   \n",
       "16                        X3   2  0.484523  0.059224  0.181147  0.208995   \n",
       "\n",
       "      above_73  norm_above_73  acc_mean_above_73  \n",
       "724       0.37       0.389284           0.002463  \n",
       "1485      0.41       0.384039           0.001595  \n",
       "1802      0.40       0.378057           0.000949  \n",
       "1474      0.38       0.348315           0.000048  \n",
       "722       0.37       0.344286           0.000031  \n",
       "...        ...            ...                ...  \n",
       "331       0.00       0.000272           0.000000  \n",
       "46        0.00       0.000234           0.000000  \n",
       "871       0.00       0.000164           0.000000  \n",
       "256       0.00       0.000036           0.000000  \n",
       "16        0.00       0.000017           0.000000  \n",
       "\n",
       "[3825 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=[\"norm_above_73\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53e7482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"../data/KNN_results_exhaust_raw6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3eaab",
   "metadata": {},
   "source": [
    "My heart warms when I see above 73 0.41 at 1485. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3df10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.27615385, 0.17615385],\n",
       "       [0.12109231, 0.4266    ]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_fold_acc=[]\n",
    "list_fold_f1=[]\n",
    "imp_record=None\n",
    "cmatrix_record=None\n",
    "n_split = 5\n",
    "n_repeats = 20\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=n_split, random_state=420, n_repeats=n_repeats)\n",
    "splits = list(RSKF.split(X=feat, y=tar))\n",
    "for train_index, test_index in splits: \n",
    "    x_tr,x_te=feat.iloc[train_index], feat.iloc[test_index]\n",
    "    y_tr, y_te=tar.iloc[train_index], tar.iloc[test_index]\n",
    "        \n",
    "    y_tr=np.ravel(y_tr.values)\n",
    "    y_te=np.ravel((y_te.values))\n",
    "        \n",
    "    KNN_pipe=Pipeline([(\"DataCreate\", training.data_creator()),(\"DataSelect\", training.data_selector(force=[\"X1\",\"X3\",\"X6\",\"above_4\"])),(\"scale\",StandardScaler()),(\"KNN\",KNeighborsClassifier(n_neighbors=1))])\n",
    "    KNN_pipe.fit(X=x_tr,y=y_tr)\n",
    "    y_p=KNN_pipe.predict(X=x_te)\n",
    "    acc=accuracy_score(y_pred=y_p,y_true=y_te)\n",
    "    f1=f1_score(y_pred=y_p,y_true=y_te)\n",
    "    list_fold_acc.append(acc)\n",
    "    list_fold_f1.append(f1)\n",
    "    # with parallel_backend(\"threading\", n_jobs=-1):\n",
    "    #     with threadpool_limits(limits=1):\n",
    "    imp=permutation_importance(KNN_pipe,X=x_te.copy(deep=True),y=y_te,scoring=\"accuracy\",n_repeats=30,n_jobs=-1,random_state=420)\n",
    "    if imp_record is None: \n",
    "        imp_record=imp.importances_mean\n",
    "    else: \n",
    "        imp_record=imp_record+imp.importances_mean\n",
    "    cmatrix=confusion_matrix(y_pred=y_p,y_true=y_te)\n",
    "    cmatrix=cmatrix/np.sum(cmatrix)\n",
    "    if cmatrix_record is None: \n",
    "        cmatrix_record=cmatrix\n",
    "    else: \n",
    "        cmatrix_record=cmatrix_record+cmatrix\n",
    "    \n",
    "\n",
    "plt.hist(list_fold_acc,bins=25)\n",
    "plt.show()\n",
    "\n",
    "imp_record=imp_record/len(splits)\n",
    "imp_sort_index=imp_record.argsort()\n",
    "plt.barh(feat.columns[imp_sort_index], imp_record[imp_sort_index])\n",
    "plt.title(\"Feature importance by permutaion method\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "cmatrix_record=cmatrix_record/len(splits)\n",
    "cmatrix_record #This is the \"Average confusion matrix\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "507b1389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float((np.array(list_fold_acc) >= 0.73).sum() / (len(splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b2e1b3",
   "metadata": {},
   "source": [
    "That is 41% sureness of beating 73% accuracy. \n",
    "\n",
    "We also can see that there is confidence of 38% that the acc of this model can beat 73%, assuming the distribution of acc is norm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad89e85",
   "metadata": {},
   "source": [
    "X5 literally has lower impact than the two features X2 and X4 that did not appear directly (they still appeared through the manufactured features). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1488f6",
   "metadata": {},
   "source": [
    "There is also the one with the highest \"acc_mean_above_73\" (724): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99bde5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.26769231, 0.18461538],\n",
       "       [0.11010769, 0.43758462]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_fold_acc=[]\n",
    "list_fold_f1=[]\n",
    "imp_record=None\n",
    "cmatrix_record=None\n",
    "n_split = 5\n",
    "n_repeats = 20\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=n_split, random_state=420, n_repeats=n_repeats)\n",
    "splits = list(RSKF.split(X=feat, y=tar))\n",
    "for train_index, test_index in splits: \n",
    "    x_tr,x_te=feat.iloc[train_index], feat.iloc[test_index]\n",
    "    y_tr, y_te=tar.iloc[train_index], tar.iloc[test_index]\n",
    "        \n",
    "    y_tr=np.ravel(y_tr.values)\n",
    "    y_te=np.ravel((y_te.values))\n",
    "        \n",
    "    KNN_pipe=Pipeline([(\"DataCreate\", training.data_creator()),(\"DataSelect\", training.data_selector(force=[\"X1\",\"X6\",\"F_w_mean\"])),(\"scale\",StandardScaler()),(\"KNN\",KNeighborsClassifier(n_neighbors=5))])\n",
    "    KNN_pipe.fit(X=x_tr,y=y_tr)\n",
    "    y_p=KNN_pipe.predict(X=x_te)\n",
    "    acc=accuracy_score(y_pred=y_p,y_true=y_te)\n",
    "    f1=f1_score(y_pred=y_p,y_true=y_te)\n",
    "    list_fold_acc.append(acc)\n",
    "    list_fold_f1.append(f1)\n",
    "    # with parallel_backend(\"threading\", n_jobs=-1):\n",
    "    #     with threadpool_limits(limits=1):\n",
    "    imp=permutation_importance(KNN_pipe,X=x_te.copy(deep=True),y=y_te,scoring=\"accuracy\",n_repeats=30,n_jobs=-1,random_state=420)\n",
    "    if imp_record is None: \n",
    "        imp_record=imp.importances_mean\n",
    "    else: \n",
    "        imp_record=imp_record+imp.importances_mean\n",
    "    cmatrix=confusion_matrix(y_pred=y_p,y_true=y_te)\n",
    "    cmatrix=cmatrix/np.sum(cmatrix)\n",
    "    if cmatrix_record is None: \n",
    "        cmatrix_record=cmatrix\n",
    "    else: \n",
    "        cmatrix_record=cmatrix_record+cmatrix\n",
    "    \n",
    "\n",
    "plt.hist(list_fold_acc,bins=25)\n",
    "plt.show()\n",
    "\n",
    "imp_record=imp_record/len(splits)\n",
    "imp_sort_index=imp_record.argsort()\n",
    "plt.barh(feat.columns[imp_sort_index], imp_record[imp_sort_index])\n",
    "plt.title(\"Feature importance by permutaion method\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "cmatrix_record=cmatrix_record/len(splits)\n",
    "cmatrix_record #This is the \"Average confusion matrix\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1234555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float((np.array(list_fold_acc) >= 0.73).sum() / (len(splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ecaf69",
   "metadata": {},
   "source": [
    "A review on the two \"best\" candidates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7d7a255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>nn</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>above_73</th>\n",
       "      <th>norm_above_73</th>\n",
       "      <th>acc_mean_above_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.705277</td>\n",
       "      <td>0.087925</td>\n",
       "      <td>0.746999</td>\n",
       "      <td>0.076984</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.389284</td>\n",
       "      <td>0.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>X1,X3,X6,above_4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702754</td>\n",
       "      <td>0.092395</td>\n",
       "      <td>0.739808</td>\n",
       "      <td>0.084133</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.384039</td>\n",
       "      <td>0.001595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  nn  acc_mean   acc_std   f1_mean    f1_std  above_73  \\\n",
       "724     X1,X6,F_w_mean   5  0.705277  0.087925  0.746999  0.076984      0.37   \n",
       "1485  X1,X3,X6,above_4   1  0.702754  0.092395  0.739808  0.084133      0.41   \n",
       "\n",
       "      norm_above_73  acc_mean_above_73  \n",
       "724        0.389284           0.002463  \n",
       "1485       0.384039           0.001595  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.iloc[[724,1485]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cd304",
   "metadata": {},
   "source": [
    "Personally, I prefer 724: \"acc_mean\", \"f1_mean\", \"norm_above_73\", and \"acc_mean_above_73\" are higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a05ab",
   "metadata": {},
   "source": [
    "I am certain that if I were to remove any raw features, it would be \"X2\" and \"X4\" at this point. I would probably let \"X4\" go first. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949798f",
   "metadata": {},
   "source": [
    "Let's do a quick hypo test: \n",
    "\n",
    "* H0: \"acc_mean\" >= 0.73 \n",
    "* Ha: \"acc_mean\" < 0.73 \n",
    "\n",
    "We will do T-test. And judge by p-value for the two \"best models\" we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "001bcbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7027538461538463)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[\"acc_mean\"].iloc[1485]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56a5718f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0019895567547459697)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1485 \n",
    "t_stat_1485=(df_results[\"acc_mean\"].iloc[1485]-0.73)/(df_results[\"acc_std\"].iloc[1485]/10)\n",
    "p_v_1485=t.cdf(t_stat_1485,99)\n",
    "p_v_1485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "876f8257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0029697829342944016)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 724\n",
    "t_stat_724=(df_results[\"acc_mean\"].iloc[724]-0.73)/(df_results[\"acc_std\"].iloc[724]/10)\n",
    "p_v_724=t.cdf(t_stat_724,99)\n",
    "p_v_724"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda2dd0e",
   "metadata": {},
   "source": [
    "Since the p value of 724 is higher than 1485, I will say I actually prefer 724 more. \n",
    "Although H0 is rejected of I use the common $\\alpha=0.05$ or even just the $\\alpha=0.01$ line, I feel this is still a pretty good model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a15d52",
   "metadata": {},
   "source": [
    "### With 5 raw feature inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496e730",
   "metadata": {},
   "source": [
    "As we have seen in above, for our \"best model\", the raw feature X4 has the least importance. Removing it might be helpful in reducing noisy, so let's try that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07431714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/raw.csv\")\n",
    "features=[feature for feature in list(df.columns)[1:] if feature != \"X4\"]\n",
    "target=[\"Y\"]\n",
    "feat5=df[features]\n",
    "tar=df[target]\n",
    "# x_t, x_v, y_t, y_v= train_test_split(feat,tar, test_size=0.2, random_state=0, stratify=tar[\"Y\"])\n",
    "n_splits=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6574671f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X3', 'X5', 'X6', 'mean', 'F_w_mean', 'above_4', 'above_5'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_pipe=Pipeline([(\"DataCreater\", training.data_creator()),(\"DataSelector\",training.data_selector())])\n",
    "tar_arr=np.ravel(tar.values)\n",
    "eva_pipe.fit(X=feat5,y=tar)\n",
    "eva_out=eva_pipe.transform(X=feat5)\n",
    "eva_out.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f11b6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>10.561708</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059797</td>\n",
       "      <td>0.283358</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>0.662751</td>\n",
       "      <td>0.838814</td>\n",
       "      <td>0.279529</td>\n",
       "      <td>0.560670</td>\n",
       "      <td>0.667312</td>\n",
       "      <td>0.280160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X3</td>\n",
       "      <td>2.886959</td>\n",
       "      <td>0.091807</td>\n",
       "      <td>0.283358</td>\n",
       "      <td>0.184129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>0.203750</td>\n",
       "      <td>0.677746</td>\n",
       "      <td>0.528224</td>\n",
       "      <td>0.480988</td>\n",
       "      <td>0.668304</td>\n",
       "      <td>0.459888</td>\n",
       "      <td>0.150838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X5</td>\n",
       "      <td>6.582716</td>\n",
       "      <td>0.011488</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>0.039996</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.320195</td>\n",
       "      <td>0.722037</td>\n",
       "      <td>0.805060</td>\n",
       "      <td>0.498525</td>\n",
       "      <td>0.638135</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.224522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.586849</td>\n",
       "      <td>0.060568</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>-0.062205</td>\n",
       "      <td>0.203750</td>\n",
       "      <td>0.320195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.548922</td>\n",
       "      <td>0.573102</td>\n",
       "      <td>0.218897</td>\n",
       "      <td>0.485442</td>\n",
       "      <td>0.543849</td>\n",
       "      <td>0.167669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mean</td>\n",
       "      <td>8.158948</td>\n",
       "      <td>0.005026</td>\n",
       "      <td>0.662751</td>\n",
       "      <td>0.449269</td>\n",
       "      <td>0.677746</td>\n",
       "      <td>0.722037</td>\n",
       "      <td>0.548922</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895558</td>\n",
       "      <td>0.700471</td>\n",
       "      <td>0.850609</td>\n",
       "      <td>0.791726</td>\n",
       "      <td>0.248467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>12.672237</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.838814</td>\n",
       "      <td>0.076364</td>\n",
       "      <td>0.528224</td>\n",
       "      <td>0.805060</td>\n",
       "      <td>0.573102</td>\n",
       "      <td>0.895558</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.501890</td>\n",
       "      <td>0.790447</td>\n",
       "      <td>0.801900</td>\n",
       "      <td>0.304499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>above_4</td>\n",
       "      <td>8.106062</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.560670</td>\n",
       "      <td>0.261968</td>\n",
       "      <td>0.668304</td>\n",
       "      <td>0.638135</td>\n",
       "      <td>0.485442</td>\n",
       "      <td>0.850609</td>\n",
       "      <td>0.790447</td>\n",
       "      <td>0.476153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.595610</td>\n",
       "      <td>0.247710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>above_5</td>\n",
       "      <td>7.658100</td>\n",
       "      <td>0.006518</td>\n",
       "      <td>0.667312</td>\n",
       "      <td>0.211873</td>\n",
       "      <td>0.459888</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.543849</td>\n",
       "      <td>0.791726</td>\n",
       "      <td>0.801900</td>\n",
       "      <td>0.258873</td>\n",
       "      <td>0.595610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.241177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features    f score   p value        X1        X2        X3        X5  \\\n",
       "0        X1  10.561708  0.001486  1.000000  0.059797  0.283358  0.432772   \n",
       "2        X3   2.886959  0.091807  0.283358  0.184129  1.000000  0.358397   \n",
       "3        X5   6.582716  0.011488  0.432772  0.039996  0.358397  1.000000   \n",
       "4        X6   3.586849  0.060568  0.411873 -0.062205  0.203750  0.320195   \n",
       "5      mean   8.158948  0.005026  0.662751  0.449269  0.677746  0.722037   \n",
       "6  F_w_mean  12.672237  0.000527  0.838814  0.076364  0.528224  0.805060   \n",
       "8   above_4   8.106062  0.005165  0.560670  0.261968  0.668304  0.638135   \n",
       "9   above_5   7.658100  0.006518  0.667312  0.211873  0.459888  0.603333   \n",
       "\n",
       "         X6      mean  F_w_mean   above_3   above_4   above_5         Y  \n",
       "0  0.411873  0.662751  0.838814  0.279529  0.560670  0.667312  0.280160  \n",
       "2  0.203750  0.677746  0.528224  0.480988  0.668304  0.459888  0.150838  \n",
       "3  0.320195  0.722037  0.805060  0.498525  0.638135  0.603333  0.224522  \n",
       "4  1.000000  0.548922  0.573102  0.218897  0.485442  0.543849  0.167669  \n",
       "5  0.548922  1.000000  0.895558  0.700471  0.850609  0.791726  0.248467  \n",
       "6  0.573102  0.895558  1.000000  0.501890  0.790447  0.801900  0.304499  \n",
       "8  0.485442  0.850609  0.790447  0.476153  1.000000  0.595610  0.247710  \n",
       "9  0.543849  0.791726  0.801900  0.258873  0.595610  1.000000  0.241177  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_pipe[\"DataSelector\"].sel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b8b0c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>10.561708</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059797</td>\n",
       "      <td>0.283358</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>0.662751</td>\n",
       "      <td>0.838814</td>\n",
       "      <td>0.279529</td>\n",
       "      <td>0.560670</td>\n",
       "      <td>0.667312</td>\n",
       "      <td>0.280160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X2</td>\n",
       "      <td>0.073108</td>\n",
       "      <td>0.787313</td>\n",
       "      <td>0.059797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.184129</td>\n",
       "      <td>0.039996</td>\n",
       "      <td>-0.062205</td>\n",
       "      <td>0.449269</td>\n",
       "      <td>0.076364</td>\n",
       "      <td>0.569174</td>\n",
       "      <td>0.261968</td>\n",
       "      <td>0.211873</td>\n",
       "      <td>-0.024274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X3</td>\n",
       "      <td>2.886959</td>\n",
       "      <td>0.091807</td>\n",
       "      <td>0.283358</td>\n",
       "      <td>0.184129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>0.203750</td>\n",
       "      <td>0.677746</td>\n",
       "      <td>0.528224</td>\n",
       "      <td>0.480988</td>\n",
       "      <td>0.668304</td>\n",
       "      <td>0.459888</td>\n",
       "      <td>0.150838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X5</td>\n",
       "      <td>6.582716</td>\n",
       "      <td>0.011488</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>0.039996</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.320195</td>\n",
       "      <td>0.722037</td>\n",
       "      <td>0.805060</td>\n",
       "      <td>0.498525</td>\n",
       "      <td>0.638135</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.224522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.586849</td>\n",
       "      <td>0.060568</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>-0.062205</td>\n",
       "      <td>0.203750</td>\n",
       "      <td>0.320195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.548922</td>\n",
       "      <td>0.573102</td>\n",
       "      <td>0.218897</td>\n",
       "      <td>0.485442</td>\n",
       "      <td>0.543849</td>\n",
       "      <td>0.167669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mean</td>\n",
       "      <td>8.158948</td>\n",
       "      <td>0.005026</td>\n",
       "      <td>0.662751</td>\n",
       "      <td>0.449269</td>\n",
       "      <td>0.677746</td>\n",
       "      <td>0.722037</td>\n",
       "      <td>0.548922</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.895558</td>\n",
       "      <td>0.700471</td>\n",
       "      <td>0.850609</td>\n",
       "      <td>0.791726</td>\n",
       "      <td>0.248467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>12.672237</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.838814</td>\n",
       "      <td>0.076364</td>\n",
       "      <td>0.528224</td>\n",
       "      <td>0.805060</td>\n",
       "      <td>0.573102</td>\n",
       "      <td>0.895558</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.501890</td>\n",
       "      <td>0.790447</td>\n",
       "      <td>0.801900</td>\n",
       "      <td>0.304499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>above_3</td>\n",
       "      <td>0.886874</td>\n",
       "      <td>0.348157</td>\n",
       "      <td>0.279529</td>\n",
       "      <td>0.569174</td>\n",
       "      <td>0.480988</td>\n",
       "      <td>0.498525</td>\n",
       "      <td>0.218897</td>\n",
       "      <td>0.700471</td>\n",
       "      <td>0.501890</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.476153</td>\n",
       "      <td>0.258873</td>\n",
       "      <td>0.084270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>above_4</td>\n",
       "      <td>8.106062</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.560670</td>\n",
       "      <td>0.261968</td>\n",
       "      <td>0.668304</td>\n",
       "      <td>0.638135</td>\n",
       "      <td>0.485442</td>\n",
       "      <td>0.850609</td>\n",
       "      <td>0.790447</td>\n",
       "      <td>0.476153</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.595610</td>\n",
       "      <td>0.247710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>above_5</td>\n",
       "      <td>7.658100</td>\n",
       "      <td>0.006518</td>\n",
       "      <td>0.667312</td>\n",
       "      <td>0.211873</td>\n",
       "      <td>0.459888</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.543849</td>\n",
       "      <td>0.791726</td>\n",
       "      <td>0.801900</td>\n",
       "      <td>0.258873</td>\n",
       "      <td>0.595610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.241177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features    f score   p value        X1        X2        X3        X5  \\\n",
       "0        X1  10.561708  0.001486  1.000000  0.059797  0.283358  0.432772   \n",
       "1        X2   0.073108  0.787313  0.059797  1.000000  0.184129  0.039996   \n",
       "2        X3   2.886959  0.091807  0.283358  0.184129  1.000000  0.358397   \n",
       "3        X5   6.582716  0.011488  0.432772  0.039996  0.358397  1.000000   \n",
       "4        X6   3.586849  0.060568  0.411873 -0.062205  0.203750  0.320195   \n",
       "5      mean   8.158948  0.005026  0.662751  0.449269  0.677746  0.722037   \n",
       "6  F_w_mean  12.672237  0.000527  0.838814  0.076364  0.528224  0.805060   \n",
       "7   above_3   0.886874  0.348157  0.279529  0.569174  0.480988  0.498525   \n",
       "8   above_4   8.106062  0.005165  0.560670  0.261968  0.668304  0.638135   \n",
       "9   above_5   7.658100  0.006518  0.667312  0.211873  0.459888  0.603333   \n",
       "\n",
       "         X6      mean  F_w_mean   above_3   above_4   above_5         Y  \n",
       "0  0.411873  0.662751  0.838814  0.279529  0.560670  0.667312  0.280160  \n",
       "1 -0.062205  0.449269  0.076364  0.569174  0.261968  0.211873 -0.024274  \n",
       "2  0.203750  0.677746  0.528224  0.480988  0.668304  0.459888  0.150838  \n",
       "3  0.320195  0.722037  0.805060  0.498525  0.638135  0.603333  0.224522  \n",
       "4  1.000000  0.548922  0.573102  0.218897  0.485442  0.543849  0.167669  \n",
       "5  0.548922  1.000000  0.895558  0.700471  0.850609  0.791726  0.248467  \n",
       "6  0.573102  0.895558  1.000000  0.501890  0.790447  0.801900  0.304499  \n",
       "7  0.218897  0.700471  0.501890  1.000000  0.476153  0.258873  0.084270  \n",
       "8  0.485442  0.850609  0.790447  0.476153  1.000000  0.595610  0.247710  \n",
       "9  0.543849  0.791726  0.801900  0.258873  0.595610  1.000000  0.241177  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_pipe[\"DataSelector\"].total_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976f723",
   "metadata": {},
   "source": [
    "We will use a pre-made evaludate_combo instead, it is cleaner. I will be honest, I do not think this will have too much impact, yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ada3e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "range_nn = range(1, 15 + 1)\n",
    "range_feat5_combin = training.all_combin(eva_out.columns)\n",
    "print(len(range_feat5_combin))\n",
    "\n",
    "n_split = 5\n",
    "n_repeats = 20\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=n_split, random_state=420, n_repeats=n_repeats)\n",
    "splits = list(RSKF.split(X=feat5, y=tar))\n",
    "\n",
    "pipe=Pipeline([\n",
    "    (\"DataCreate\", training.data_creator()),\n",
    "    (\"DataSelector\", training.data_selector()),\n",
    "    (\"scale\",StandardScaler()),\n",
    "    (\"KNN\",KNeighborsClassifier())]\n",
    ")\n",
    "\n",
    "jobs5 = list(itertools.product(range_feat5_combin, range_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "daebdb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   45.3s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   48.1s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1426 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1481 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1536 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1593 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1650 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1709 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1829 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1890 tasks      | elapsed: 15.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1953 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2016 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2081 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2146 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2213 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2280 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2349 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2489 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2560 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2633 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2706 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2781 tasks      | elapsed: 22.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2856 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2933 tasks      | elapsed: 23.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3010 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3089 tasks      | elapsed: 24.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3249 tasks      | elapsed: 25.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3330 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3413 tasks      | elapsed: 27.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3496 tasks      | elapsed: 27.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3581 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3666 tasks      | elapsed: 29.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3753 tasks      | elapsed: 29.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3825 out of 3825 | elapsed: 30.4min finished\n"
     ]
    }
   ],
   "source": [
    "results = Parallel(n_jobs=-1, backend=\"loky\", verbose=10)(\n",
    "    delayed(training.evaluate_combo)(\n",
    "        list_f_sel_tuple=feat_sel, \n",
    "        dict_param={\"KNN__n_neighbors\": nn}, \n",
    "        splits=splits, \n",
    "        pipe=pipe, \n",
    "        feat=feat5, \n",
    "        tar=tar \n",
    "    )\n",
    "    for feat_sel, nn in jobs5\n",
    ")\n",
    "\n",
    "list_feat      = [r[\"features\"] for r in results]\n",
    "list_nn        = [r[\"KNN__n_neighbors\"] for r in results] \n",
    "list_acc_mean  = [r[\"acc_mean\"] for r in results]\n",
    "list_acc_std   = [r[\"acc_std\"] for r in results]\n",
    "list_f1_mean   = [r[\"f1_mean\"] for r in results]\n",
    "list_f1_std    = [r[\"f1_std\"] for r in results]\n",
    "list_above_73  = [r[\"above_73\"] for r in results]\n",
    "list_norm_above_73 = [r[\"norm_above_73\"] for r in results] \n",
    "list_acc_mean_above_73 = [r[\"acc_mean_above_73\"] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abcf3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({\n",
    "    #Hyper-parameters\n",
    "    \"features\": list_feat, \n",
    "    \"nn\": list_nn, \n",
    "    #Performances\n",
    "    \"acc_mean\": list_acc_mean,\n",
    "    \"acc_std\": list_acc_std,\n",
    "    \"f1_mean\": list_f1_mean,\n",
    "    \"f1_std\": list_f1_std,\n",
    "    \"above_73\": list_above_73,\n",
    "    \"norm_above_73\": list_norm_above_73, \n",
    "    \"acc_mean_above_73\": list_acc_mean_above_73\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf19093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"../data/KNN_results_exhaust_raw5.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48de5b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>nn</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>above_73</th>\n",
       "      <th>norm_above_73</th>\n",
       "      <th>acc_mean_above_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>X1,X6,above_4,above_5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.705923</td>\n",
       "      <td>0.086046</td>\n",
       "      <td>0.744130</td>\n",
       "      <td>0.078033</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.389810</td>\n",
       "      <td>0.002570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.704908</td>\n",
       "      <td>0.088250</td>\n",
       "      <td>0.744359</td>\n",
       "      <td>0.077082</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.388078</td>\n",
       "      <td>0.002232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834</th>\n",
       "      <td>X1,X6,above_4,above_5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.703554</td>\n",
       "      <td>0.088737</td>\n",
       "      <td>0.734543</td>\n",
       "      <td>0.081431</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.382840</td>\n",
       "      <td>0.001440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>X1,X6,F_w_mean,above_4,above_5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.698646</td>\n",
       "      <td>0.090634</td>\n",
       "      <td>0.712607</td>\n",
       "      <td>0.088653</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.364695</td>\n",
       "      <td>0.000271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>7</td>\n",
       "      <td>0.695615</td>\n",
       "      <td>0.098137</td>\n",
       "      <td>0.733482</td>\n",
       "      <td>0.089972</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.363029</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>X6,F_w_mean,above_5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.580492</td>\n",
       "      <td>0.083992</td>\n",
       "      <td>0.620862</td>\n",
       "      <td>0.079355</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.037536</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>X6,F_w_mean,above_5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.511646</td>\n",
       "      <td>0.078233</td>\n",
       "      <td>0.446644</td>\n",
       "      <td>0.104474</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>X6,F_w_mean,above_5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.637092</td>\n",
       "      <td>0.096423</td>\n",
       "      <td>0.676370</td>\n",
       "      <td>0.090096</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.167637</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>X6,F_w_mean,above_5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.610031</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>0.603231</td>\n",
       "      <td>0.110808</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.111634</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>X6,mean,above_5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.615031</td>\n",
       "      <td>0.084825</td>\n",
       "      <td>0.680091</td>\n",
       "      <td>0.077088</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.087649</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3825 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            features  nn  acc_mean   acc_std   f1_mean  \\\n",
       "1832           X1,X6,above_4,above_5   3  0.705923  0.086046  0.744130   \n",
       "724                   X1,X6,F_w_mean   5  0.704908  0.088250  0.744359   \n",
       "1834           X1,X6,above_4,above_5   5  0.703554  0.088737  0.734543   \n",
       "2930  X1,X6,F_w_mean,above_4,above_5   6  0.698646  0.090634  0.712607   \n",
       "726                   X1,X6,F_w_mean   7  0.695615  0.098137  0.733482   \n",
       "...                              ...  ..       ...       ...       ...   \n",
       "1290             X6,F_w_mean,above_5   1  0.580492  0.083992  0.620862   \n",
       "1291             X6,F_w_mean,above_5   2  0.511646  0.078233  0.446644   \n",
       "1292             X6,F_w_mean,above_5   3  0.637092  0.096423  0.676370   \n",
       "1293             X6,F_w_mean,above_5   4  0.610031  0.098506  0.603231   \n",
       "1272                 X6,mean,above_5  13  0.615031  0.084825  0.680091   \n",
       "\n",
       "        f1_std  above_73  norm_above_73  acc_mean_above_73  \n",
       "1832  0.078033      0.39       0.389810           0.002570  \n",
       "724   0.077082      0.37       0.388078           0.002232  \n",
       "1834  0.081431      0.37       0.382840           0.001440  \n",
       "2930  0.088653      0.35       0.364695           0.000271  \n",
       "726   0.089972      0.38       0.363029           0.000229  \n",
       "...        ...       ...            ...                ...  \n",
       "1290  0.079355      0.04       0.037536           0.000000  \n",
       "1291  0.104474      0.00       0.002627           0.000000  \n",
       "1292  0.090096      0.18       0.167637           0.000000  \n",
       "1293  0.110808      0.09       0.111634           0.000000  \n",
       "1272  0.077088      0.08       0.087649           0.000000  \n",
       "\n",
       "[3825 rows x 9 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=[\"acc_mean_above_73\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5b58712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>nn</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>above_73</th>\n",
       "      <th>norm_above_73</th>\n",
       "      <th>acc_mean_above_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3579</th>\n",
       "      <td>X1,X6,mean,F_w_mean,above_4,above_5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.679277</td>\n",
       "      <td>0.105989</td>\n",
       "      <td>0.717668</td>\n",
       "      <td>0.093276</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.316122</td>\n",
       "      <td>8.519601e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>14</td>\n",
       "      <td>0.679908</td>\n",
       "      <td>0.090463</td>\n",
       "      <td>0.721931</td>\n",
       "      <td>0.079370</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.289881</td>\n",
       "      <td>1.535660e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>10</td>\n",
       "      <td>0.675077</td>\n",
       "      <td>0.097064</td>\n",
       "      <td>0.708256</td>\n",
       "      <td>0.089008</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.285750</td>\n",
       "      <td>7.638095e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>X1,X6,mean,F_w_mean,above_4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.675785</td>\n",
       "      <td>0.094924</td>\n",
       "      <td>0.710456</td>\n",
       "      <td>0.085605</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.283951</td>\n",
       "      <td>5.601523e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>12</td>\n",
       "      <td>0.679108</td>\n",
       "      <td>0.088423</td>\n",
       "      <td>0.718047</td>\n",
       "      <td>0.077022</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.282459</td>\n",
       "      <td>4.318736e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>X6,mean,above_5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.615031</td>\n",
       "      <td>0.084825</td>\n",
       "      <td>0.680091</td>\n",
       "      <td>0.077088</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.087649</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>X6,mean,above_5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.600877</td>\n",
       "      <td>0.093310</td>\n",
       "      <td>0.636033</td>\n",
       "      <td>0.105784</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.083208</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>X6,mean,above_5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.609092</td>\n",
       "      <td>0.093733</td>\n",
       "      <td>0.663419</td>\n",
       "      <td>0.097521</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.098539</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>X6,mean,above_5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.586938</td>\n",
       "      <td>0.091483</td>\n",
       "      <td>0.610715</td>\n",
       "      <td>0.107012</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.058931</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>X6,F_w_mean,above_5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.618200</td>\n",
       "      <td>0.088945</td>\n",
       "      <td>0.667773</td>\n",
       "      <td>0.094841</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.104385</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1530 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 features  nn  acc_mean   acc_std   f1_mean  \\\n",
       "3579  X1,X6,mean,F_w_mean,above_4,above_5  10  0.679277  0.105989  0.717668   \n",
       "733                        X1,X6,F_w_mean  14  0.679908  0.090463  0.721931   \n",
       "729                        X1,X6,F_w_mean  10  0.675077  0.097064  0.708256   \n",
       "2889          X1,X6,mean,F_w_mean,above_4  10  0.675785  0.094924  0.710456   \n",
       "731                        X1,X6,F_w_mean  12  0.679108  0.088423  0.718047   \n",
       "...                                   ...  ..       ...       ...       ...   \n",
       "1272                      X6,mean,above_5  13  0.615031  0.084825  0.680091   \n",
       "1271                      X6,mean,above_5  12  0.600877  0.093310  0.636033   \n",
       "1270                      X6,mean,above_5  11  0.609092  0.093733  0.663419   \n",
       "1269                      X6,mean,above_5  10  0.586938  0.091483  0.610715   \n",
       "1300                  X6,F_w_mean,above_5  11  0.618200  0.088945  0.667773   \n",
       "\n",
       "        f1_std  above_73  norm_above_73  acc_mean_above_73  \n",
       "3579  0.093276      0.33       0.316122       8.519601e-07  \n",
       "733   0.079370      0.34       0.289881       1.535660e-08  \n",
       "729   0.089008      0.29       0.285750       7.638095e-09  \n",
       "2889  0.085605      0.30       0.283951       5.601523e-09  \n",
       "731   0.077022      0.28       0.282459       4.318736e-09  \n",
       "...        ...       ...            ...                ...  \n",
       "1272  0.077088      0.08       0.087649       0.000000e+00  \n",
       "1271  0.105784      0.06       0.083208       0.000000e+00  \n",
       "1270  0.097521      0.11       0.098539       0.000000e+00  \n",
       "1269  0.107012      0.06       0.058931       0.000000e+00  \n",
       "1300  0.094841      0.13       0.104385       0.000000e+00  \n",
       "\n",
       "[1530 rows x 9 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[df_results[\"nn\"]>=10].sort_values(by=[\"acc_mean_above_73\"],ascending=False) #I want to see if the nn >= 10 is worth it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723cc2fe",
   "metadata": {},
   "source": [
    "1832 produced my favorite model. We will investigate further into that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfd9e0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.27463077 0.17767692]\n",
      " [0.1164     0.43129231]]\n"
     ]
    }
   ],
   "source": [
    "n_split = 5 \n",
    "n_repeats = 20\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=n_split, random_state=420, n_repeats=n_repeats)\n",
    "splits = list(RSKF.split(X=feat5, y=tar))\n",
    "\n",
    "pipe=Pipeline([\n",
    "    (\"DataCreate\", training.data_creator()),\n",
    "    (\"DataSelector\", training.data_selector(force=[\"X1\",\"X6\",\"above_4\",\"above_5\"])),\n",
    "    (\"scale\",StandardScaler()),\n",
    "    (\"KNN\",KNeighborsClassifier(n_neighbors=3))]\n",
    ")\n",
    "\n",
    "training.show_result(splits=splits, pipe=pipe, feat=feat5, tar=tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba86629",
   "metadata": {},
   "source": [
    "We will further remove X2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe5a7b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X3', 'X5', 'X6', 'mean', 'F_w_mean', 'above_3', 'above_4',\n",
       "       'above_5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"../data/raw.csv\")\n",
    "features=[feature for feature in list(df.columns)[1:] if feature not in [\"X4\", \"X2\"]]\n",
    "target=[\"Y\"]\n",
    "feat4=df[features]\n",
    "tar=df[target]\n",
    "# x_t, x_v, y_t, y_v= train_test_split(feat,tar, test_size=0.2, random_state=0, stratify=tar[\"Y\"])\n",
    "n_splits=5\n",
    "\n",
    "eva_pipe=Pipeline([(\"DataCreater\", training.data_creator()),(\"DataSelector\",training.data_selector())])\n",
    "tar_arr=np.ravel(tar.values)\n",
    "eva_pipe.fit(X=feat4,y=tar)\n",
    "eva_out=eva_pipe.transform(X=feat4)\n",
    "eva_out.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2528d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "      <th>X1</th>\n",
       "      <th>X3</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>10.561708</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283358</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>0.714267</td>\n",
       "      <td>0.838825</td>\n",
       "      <td>0.367825</td>\n",
       "      <td>0.580152</td>\n",
       "      <td>0.683732</td>\n",
       "      <td>0.280160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X3</td>\n",
       "      <td>2.886959</td>\n",
       "      <td>0.091807</td>\n",
       "      <td>0.283358</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>0.203750</td>\n",
       "      <td>0.679422</td>\n",
       "      <td>0.527503</td>\n",
       "      <td>0.590444</td>\n",
       "      <td>0.632243</td>\n",
       "      <td>0.462141</td>\n",
       "      <td>0.150838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X5</td>\n",
       "      <td>6.582716</td>\n",
       "      <td>0.011488</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.320195</td>\n",
       "      <td>0.788594</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>0.651425</td>\n",
       "      <td>0.684307</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.224522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.586849</td>\n",
       "      <td>0.060568</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>0.203750</td>\n",
       "      <td>0.320195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.637932</td>\n",
       "      <td>0.573621</td>\n",
       "      <td>0.319799</td>\n",
       "      <td>0.546312</td>\n",
       "      <td>0.587815</td>\n",
       "      <td>0.167669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>11.142964</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.714267</td>\n",
       "      <td>0.679422</td>\n",
       "      <td>0.788594</td>\n",
       "      <td>0.637932</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966938</td>\n",
       "      <td>0.708673</td>\n",
       "      <td>0.870014</td>\n",
       "      <td>0.819870</td>\n",
       "      <td>0.287147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>12.693583</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.838825</td>\n",
       "      <td>0.527503</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>0.573621</td>\n",
       "      <td>0.966938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.652805</td>\n",
       "      <td>0.825032</td>\n",
       "      <td>0.824128</td>\n",
       "      <td>0.304732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>above_3</td>\n",
       "      <td>2.818124</td>\n",
       "      <td>0.095724</td>\n",
       "      <td>0.367825</td>\n",
       "      <td>0.590444</td>\n",
       "      <td>0.651425</td>\n",
       "      <td>0.319799</td>\n",
       "      <td>0.708673</td>\n",
       "      <td>0.652805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.553929</td>\n",
       "      <td>0.315653</td>\n",
       "      <td>0.149070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>above_4</td>\n",
       "      <td>11.298950</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.580152</td>\n",
       "      <td>0.632243</td>\n",
       "      <td>0.684307</td>\n",
       "      <td>0.546312</td>\n",
       "      <td>0.870014</td>\n",
       "      <td>0.825032</td>\n",
       "      <td>0.553929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571427</td>\n",
       "      <td>0.288983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>above_5</td>\n",
       "      <td>8.032573</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.683732</td>\n",
       "      <td>0.462141</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.587815</td>\n",
       "      <td>0.819870</td>\n",
       "      <td>0.824128</td>\n",
       "      <td>0.315653</td>\n",
       "      <td>0.571427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.246653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features    f score   p value        X1        X3        X5        X6  \\\n",
       "0        X1  10.561708  0.001486  1.000000  0.283358  0.432772  0.411873   \n",
       "1        X3   2.886959  0.091807  0.283358  1.000000  0.358397  0.203750   \n",
       "2        X5   6.582716  0.011488  0.432772  0.358397  1.000000  0.320195   \n",
       "3        X6   3.586849  0.060568  0.411873  0.203750  0.320195  1.000000   \n",
       "4      mean  11.142964  0.001114  0.714267  0.679422  0.788594  0.637932   \n",
       "5  F_w_mean  12.693583  0.000522  0.838825  0.527503  0.805157  0.573621   \n",
       "6   above_3   2.818124  0.095724  0.367825  0.590444  0.651425  0.319799   \n",
       "7   above_4  11.298950  0.001031  0.580152  0.632243  0.684307  0.546312   \n",
       "8   above_5   8.032573  0.005365  0.683732  0.462141  0.617076  0.587815   \n",
       "\n",
       "       mean  F_w_mean   above_3   above_4   above_5         Y  \n",
       "0  0.714267  0.838825  0.367825  0.580152  0.683732  0.280160  \n",
       "1  0.679422  0.527503  0.590444  0.632243  0.462141  0.150838  \n",
       "2  0.788594  0.805157  0.651425  0.684307  0.617076  0.224522  \n",
       "3  0.637932  0.573621  0.319799  0.546312  0.587815  0.167669  \n",
       "4  1.000000  0.966938  0.708673  0.870014  0.819870  0.287147  \n",
       "5  0.966938  1.000000  0.652805  0.825032  0.824128  0.304732  \n",
       "6  0.708673  0.652805  1.000000  0.553929  0.315653  0.149070  \n",
       "7  0.870014  0.825032  0.553929  1.000000  0.571427  0.288983  \n",
       "8  0.819870  0.824128  0.315653  0.571427  1.000000  0.246653  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_pipe[\"DataSelector\"].sel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c076a882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>f score</th>\n",
       "      <th>p value</th>\n",
       "      <th>X1</th>\n",
       "      <th>X3</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>mean</th>\n",
       "      <th>F_w_mean</th>\n",
       "      <th>above_3</th>\n",
       "      <th>above_4</th>\n",
       "      <th>above_5</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>10.561708</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283358</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>0.714267</td>\n",
       "      <td>0.838825</td>\n",
       "      <td>0.367825</td>\n",
       "      <td>0.580152</td>\n",
       "      <td>0.683732</td>\n",
       "      <td>0.280160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X3</td>\n",
       "      <td>2.886959</td>\n",
       "      <td>0.091807</td>\n",
       "      <td>0.283358</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>0.203750</td>\n",
       "      <td>0.679422</td>\n",
       "      <td>0.527503</td>\n",
       "      <td>0.590444</td>\n",
       "      <td>0.632243</td>\n",
       "      <td>0.462141</td>\n",
       "      <td>0.150838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X5</td>\n",
       "      <td>6.582716</td>\n",
       "      <td>0.011488</td>\n",
       "      <td>0.432772</td>\n",
       "      <td>0.358397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.320195</td>\n",
       "      <td>0.788594</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>0.651425</td>\n",
       "      <td>0.684307</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.224522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X6</td>\n",
       "      <td>3.586849</td>\n",
       "      <td>0.060568</td>\n",
       "      <td>0.411873</td>\n",
       "      <td>0.203750</td>\n",
       "      <td>0.320195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.637932</td>\n",
       "      <td>0.573621</td>\n",
       "      <td>0.319799</td>\n",
       "      <td>0.546312</td>\n",
       "      <td>0.587815</td>\n",
       "      <td>0.167669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>11.142964</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.714267</td>\n",
       "      <td>0.679422</td>\n",
       "      <td>0.788594</td>\n",
       "      <td>0.637932</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966938</td>\n",
       "      <td>0.708673</td>\n",
       "      <td>0.870014</td>\n",
       "      <td>0.819870</td>\n",
       "      <td>0.287147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F_w_mean</td>\n",
       "      <td>12.693583</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.838825</td>\n",
       "      <td>0.527503</td>\n",
       "      <td>0.805157</td>\n",
       "      <td>0.573621</td>\n",
       "      <td>0.966938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.652805</td>\n",
       "      <td>0.825032</td>\n",
       "      <td>0.824128</td>\n",
       "      <td>0.304732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>above_3</td>\n",
       "      <td>2.818124</td>\n",
       "      <td>0.095724</td>\n",
       "      <td>0.367825</td>\n",
       "      <td>0.590444</td>\n",
       "      <td>0.651425</td>\n",
       "      <td>0.319799</td>\n",
       "      <td>0.708673</td>\n",
       "      <td>0.652805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.553929</td>\n",
       "      <td>0.315653</td>\n",
       "      <td>0.149070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>above_4</td>\n",
       "      <td>11.298950</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.580152</td>\n",
       "      <td>0.632243</td>\n",
       "      <td>0.684307</td>\n",
       "      <td>0.546312</td>\n",
       "      <td>0.870014</td>\n",
       "      <td>0.825032</td>\n",
       "      <td>0.553929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.571427</td>\n",
       "      <td>0.288983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>above_5</td>\n",
       "      <td>8.032573</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.683732</td>\n",
       "      <td>0.462141</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.587815</td>\n",
       "      <td>0.819870</td>\n",
       "      <td>0.824128</td>\n",
       "      <td>0.315653</td>\n",
       "      <td>0.571427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.246653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features    f score   p value        X1        X3        X5        X6  \\\n",
       "0        X1  10.561708  0.001486  1.000000  0.283358  0.432772  0.411873   \n",
       "1        X3   2.886959  0.091807  0.283358  1.000000  0.358397  0.203750   \n",
       "2        X5   6.582716  0.011488  0.432772  0.358397  1.000000  0.320195   \n",
       "3        X6   3.586849  0.060568  0.411873  0.203750  0.320195  1.000000   \n",
       "4      mean  11.142964  0.001114  0.714267  0.679422  0.788594  0.637932   \n",
       "5  F_w_mean  12.693583  0.000522  0.838825  0.527503  0.805157  0.573621   \n",
       "6   above_3   2.818124  0.095724  0.367825  0.590444  0.651425  0.319799   \n",
       "7   above_4  11.298950  0.001031  0.580152  0.632243  0.684307  0.546312   \n",
       "8   above_5   8.032573  0.005365  0.683732  0.462141  0.617076  0.587815   \n",
       "\n",
       "       mean  F_w_mean   above_3   above_4   above_5         Y  \n",
       "0  0.714267  0.838825  0.367825  0.580152  0.683732  0.280160  \n",
       "1  0.679422  0.527503  0.590444  0.632243  0.462141  0.150838  \n",
       "2  0.788594  0.805157  0.651425  0.684307  0.617076  0.224522  \n",
       "3  0.637932  0.573621  0.319799  0.546312  0.587815  0.167669  \n",
       "4  1.000000  0.966938  0.708673  0.870014  0.819870  0.287147  \n",
       "5  0.966938  1.000000  0.652805  0.825032  0.824128  0.304732  \n",
       "6  0.708673  0.652805  1.000000  0.553929  0.315653  0.149070  \n",
       "7  0.870014  0.825032  0.553929  1.000000  0.571427  0.288983  \n",
       "8  0.819870  0.824128  0.315653  0.571427  1.000000  0.246653  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_pipe[\"DataSelector\"].total_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581f219",
   "metadata": {},
   "source": [
    "A key difference has shown its face: above_3 how have higher than 1 f score and under 0.1 p value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0aeb968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511\n"
     ]
    }
   ],
   "source": [
    "range_nn = range(1, 15 + 1)\n",
    "# range_nn = range(10+1) # We will reduce the max nn number: All our performers never even touch 10. \n",
    "range_feat4_combin = training.all_combin(eva_out.columns)\n",
    "print(len(range_feat4_combin))\n",
    "\n",
    "n_split = 5\n",
    "n_repeats = 20\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=n_split, random_state=420, n_repeats=n_repeats)\n",
    "splits = list(RSKF.split(X=feat4, y=tar))\n",
    "\n",
    "pipe=Pipeline([\n",
    "    (\"DataCreate\", training.data_creator()),\n",
    "    (\"DataSelector\", training.data_selector()),\n",
    "    (\"scale\",StandardScaler()),\n",
    "    (\"KNN\",KNeighborsClassifier())]\n",
    ")\n",
    "\n",
    "jobs4 = list(itertools.product(range_feat4_combin, range_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd7214a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: THIS WILL TAKE A LONG TIME, THIS HAS 7665 TASKS!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   41.6s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   48.8s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   52.0s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1169 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1269 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1320 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1373 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1426 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1481 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1536 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1593 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1650 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1709 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1829 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1890 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1953 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2016 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2081 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2146 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2213 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2280 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2349 tasks      | elapsed: 19.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2489 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2560 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2633 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2706 tasks      | elapsed: 21.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2781 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2856 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2933 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3010 tasks      | elapsed: 24.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3089 tasks      | elapsed: 24.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3249 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3330 tasks      | elapsed: 26.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3413 tasks      | elapsed: 27.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3496 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3581 tasks      | elapsed: 28.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3666 tasks      | elapsed: 29.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3753 tasks      | elapsed: 30.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3840 tasks      | elapsed: 30.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3929 tasks      | elapsed: 31.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed: 32.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4109 tasks      | elapsed: 32.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4200 tasks      | elapsed: 33.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4293 tasks      | elapsed: 34.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4386 tasks      | elapsed: 35.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4481 tasks      | elapsed: 35.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4576 tasks      | elapsed: 36.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4673 tasks      | elapsed: 37.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4770 tasks      | elapsed: 38.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4869 tasks      | elapsed: 39.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4968 tasks      | elapsed: 39.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5069 tasks      | elapsed: 40.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5170 tasks      | elapsed: 41.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5273 tasks      | elapsed: 42.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5376 tasks      | elapsed: 43.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5481 tasks      | elapsed: 43.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5586 tasks      | elapsed: 44.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5693 tasks      | elapsed: 45.5min\n",
      "[Parallel(n_jobs=-1)]: Done 5800 tasks      | elapsed: 46.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5909 tasks      | elapsed: 47.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6018 tasks      | elapsed: 48.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6129 tasks      | elapsed: 49.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6240 tasks      | elapsed: 49.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6353 tasks      | elapsed: 50.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6466 tasks      | elapsed: 51.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6581 tasks      | elapsed: 52.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6696 tasks      | elapsed: 53.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6813 tasks      | elapsed: 54.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6930 tasks      | elapsed: 55.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7049 tasks      | elapsed: 56.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7168 tasks      | elapsed: 57.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7289 tasks      | elapsed: 58.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7410 tasks      | elapsed: 59.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7533 tasks      | elapsed: 60.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7665 out of 7665 | elapsed: 61.3min finished\n"
     ]
    }
   ],
   "source": [
    "#WARNING: THIS WILL TAKE A LONG TIME. \n",
    "print(\"WARNING: THIS WILL TAKE A LONG TIME, THIS HAS 7665 TASKS!!!!\")\n",
    "results = Parallel(n_jobs=-1, backend=\"loky\", verbose=10)(\n",
    "    delayed(training.evaluate_combo)(\n",
    "        list_f_sel_tuple=feat_sel, \n",
    "        dict_param={\"KNN__n_neighbors\": nn}, \n",
    "        splits=splits, \n",
    "        pipe=pipe, \n",
    "        feat=feat4, \n",
    "        tar=tar \n",
    "    )\n",
    "    for feat_sel, nn in jobs4\n",
    ")\n",
    "\n",
    "list_feat      = [r[\"features\"] for r in results]\n",
    "list_nn        = [r[\"KNN__n_neighbors\"] for r in results] \n",
    "list_acc_mean  = [r[\"acc_mean\"] for r in results]\n",
    "list_acc_std   = [r[\"acc_std\"] for r in results]\n",
    "list_f1_mean   = [r[\"f1_mean\"] for r in results]\n",
    "list_f1_std    = [r[\"f1_std\"] for r in results]\n",
    "list_above_73  = [r[\"above_73\"] for r in results]\n",
    "list_norm_above_73 = [r[\"norm_above_73\"] for r in results] \n",
    "list_acc_mean_above_73 = [r[\"acc_mean_above_73\"] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "511a1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({\n",
    "    #Hyper-parameters\n",
    "    \"features\": list_feat, \n",
    "    \"nn\": list_nn, \n",
    "    #Performances\n",
    "    \"acc_mean\": list_acc_mean,\n",
    "    \"acc_std\": list_acc_std,\n",
    "    \"f1_mean\": list_f1_mean,\n",
    "    \"f1_std\": list_f1_std,\n",
    "    \"above_73\": list_above_73,\n",
    "    \"norm_above_73\": list_norm_above_73, \n",
    "    \"acc_mean_above_73\": list_acc_mean_above_73\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75046572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"../data/KNN_results_exhaust_raw4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0580950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>nn</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>above_73</th>\n",
       "      <th>norm_above_73</th>\n",
       "      <th>acc_mean_above_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>X1,X6,F_w_mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.698954</td>\n",
       "      <td>0.093026</td>\n",
       "      <td>0.738346</td>\n",
       "      <td>0.088502</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.369289</td>\n",
       "      <td>0.000423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>X1,X6,mean</td>\n",
       "      <td>3</td>\n",
       "      <td>0.699015</td>\n",
       "      <td>0.087046</td>\n",
       "      <td>0.732929</td>\n",
       "      <td>0.082530</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.360936</td>\n",
       "      <td>0.000186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>X1,X6,above_4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.695154</td>\n",
       "      <td>0.094886</td>\n",
       "      <td>0.730803</td>\n",
       "      <td>0.087861</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.356719</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>X1,X6,mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.694231</td>\n",
       "      <td>0.094900</td>\n",
       "      <td>0.731550</td>\n",
       "      <td>0.088334</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.353118</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4189</th>\n",
       "      <td>X1,X3,X6,above_4,above_5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.694138</td>\n",
       "      <td>0.092134</td>\n",
       "      <td>0.734578</td>\n",
       "      <td>0.082199</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.348553</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>X1,mean,F_w_mean,above_3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.554215</td>\n",
       "      <td>0.087204</td>\n",
       "      <td>0.601892</td>\n",
       "      <td>0.092893</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.021911</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>X1,mean,F_w_mean,above_3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.565538</td>\n",
       "      <td>0.089571</td>\n",
       "      <td>0.567334</td>\n",
       "      <td>0.105975</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.033172</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>X1,mean,F_w_mean,above_3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.545508</td>\n",
       "      <td>0.079891</td>\n",
       "      <td>0.593397</td>\n",
       "      <td>0.093853</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010463</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>X1,mean,F_w_mean,above_3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.559692</td>\n",
       "      <td>0.088675</td>\n",
       "      <td>0.562019</td>\n",
       "      <td>0.104475</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.027393</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>X1,mean,F_w_mean,above_4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.606385</td>\n",
       "      <td>0.098621</td>\n",
       "      <td>0.647334</td>\n",
       "      <td>0.100631</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.105022</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7665 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      features  nn  acc_mean   acc_std   f1_mean    f1_std  \\\n",
       "889             X1,X6,F_w_mean   5  0.698954  0.093026  0.738346  0.088502   \n",
       "872                 X1,X6,mean   3  0.699015  0.087046  0.732929  0.082530   \n",
       "919              X1,X6,above_4   5  0.695154  0.094886  0.730803  0.087861   \n",
       "874                 X1,X6,mean   5  0.694231  0.094900  0.731550  0.088334   \n",
       "4189  X1,X3,X6,above_4,above_5   5  0.694138  0.092134  0.734578  0.082199   \n",
       "...                        ...  ..       ...       ...       ...       ...   \n",
       "2633  X1,mean,F_w_mean,above_3   9  0.554215  0.087204  0.601892  0.092893   \n",
       "2632  X1,mean,F_w_mean,above_3   8  0.565538  0.089571  0.567334  0.105975   \n",
       "2631  X1,mean,F_w_mean,above_3   7  0.545508  0.079891  0.593397  0.093853   \n",
       "2630  X1,mean,F_w_mean,above_3   6  0.559692  0.088675  0.562019  0.104475   \n",
       "2644  X1,mean,F_w_mean,above_4   5  0.606385  0.098621  0.647334  0.100631   \n",
       "\n",
       "      above_73  norm_above_73  acc_mean_above_73  \n",
       "889       0.32       0.369289           0.000423  \n",
       "872       0.34       0.360936           0.000186  \n",
       "919       0.39       0.356719           0.000120  \n",
       "874       0.37       0.353118           0.000082  \n",
       "4189      0.35       0.348553           0.000050  \n",
       "...        ...            ...                ...  \n",
       "2633      0.01       0.021911           0.000000  \n",
       "2632      0.01       0.033172           0.000000  \n",
       "2631      0.01       0.010463           0.000000  \n",
       "2630      0.03       0.027393           0.000000  \n",
       "2644      0.13       0.105022           0.000000  \n",
       "\n",
       "[7665 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=[\"acc_mean_above_73\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c9e895c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>nn</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_std</th>\n",
       "      <th>above_73</th>\n",
       "      <th>norm_above_73</th>\n",
       "      <th>acc_mean_above_73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>X1,X6,above_4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.695154</td>\n",
       "      <td>0.094886</td>\n",
       "      <td>0.730803</td>\n",
       "      <td>0.087861</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.356719</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>X1,X3,X6,mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.687938</td>\n",
       "      <td>0.099867</td>\n",
       "      <td>0.715489</td>\n",
       "      <td>0.097903</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.336813</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>X1,X6,mean</td>\n",
       "      <td>5</td>\n",
       "      <td>0.694231</td>\n",
       "      <td>0.094900</td>\n",
       "      <td>0.731550</td>\n",
       "      <td>0.088334</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.353118</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>X1,X6,mean</td>\n",
       "      <td>7</td>\n",
       "      <td>0.689462</td>\n",
       "      <td>0.103761</td>\n",
       "      <td>0.724171</td>\n",
       "      <td>0.097056</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.348013</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>X1,X3,X6,above_4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.695892</td>\n",
       "      <td>0.087444</td>\n",
       "      <td>0.730856</td>\n",
       "      <td>0.079526</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.348249</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>X3,X5,X6,mean</td>\n",
       "      <td>2</td>\n",
       "      <td>0.547385</td>\n",
       "      <td>0.087616</td>\n",
       "      <td>0.484785</td>\n",
       "      <td>0.130310</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.018568</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>X3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.514262</td>\n",
       "      <td>0.078120</td>\n",
       "      <td>0.385779</td>\n",
       "      <td>0.211094</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>X3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.524062</td>\n",
       "      <td>0.084718</td>\n",
       "      <td>0.455358</td>\n",
       "      <td>0.204684</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.007531</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>X3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.501385</td>\n",
       "      <td>0.083583</td>\n",
       "      <td>0.317555</td>\n",
       "      <td>0.213414</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.003117</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>X3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.522538</td>\n",
       "      <td>0.086813</td>\n",
       "      <td>0.435730</td>\n",
       "      <td>0.200585</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.008430</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7665 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              features  nn  acc_mean   acc_std   f1_mean    f1_std  above_73  \\\n",
       "919      X1,X6,above_4   5  0.695154  0.094886  0.730803  0.087861      0.39   \n",
       "2029     X1,X3,X6,mean   5  0.687938  0.099867  0.715489  0.097903      0.37   \n",
       "874         X1,X6,mean   5  0.694231  0.094900  0.731550  0.088334      0.37   \n",
       "876         X1,X6,mean   7  0.689462  0.103761  0.724171  0.097056      0.37   \n",
       "2074  X1,X3,X6,above_4   5  0.695892  0.087444  0.730856  0.079526      0.35   \n",
       "...                ...  ..       ...       ...       ...       ...       ...   \n",
       "2776     X3,X5,X6,mean   2  0.547385  0.087616  0.484785  0.130310      0.00   \n",
       "20                  X3   6  0.514262  0.078120  0.385779  0.211094      0.00   \n",
       "19                  X3   5  0.524062  0.084718  0.455358  0.204684      0.00   \n",
       "18                  X3   4  0.501385  0.083583  0.317555  0.213414      0.00   \n",
       "17                  X3   3  0.522538  0.086813  0.435730  0.200585      0.00   \n",
       "\n",
       "      norm_above_73  acc_mean_above_73  \n",
       "919        0.356719           0.000120  \n",
       "2029       0.336813           0.000013  \n",
       "874        0.353118           0.000082  \n",
       "876        0.348013           0.000047  \n",
       "2074       0.348249           0.000048  \n",
       "...             ...                ...  \n",
       "2776       0.018568           0.000000  \n",
       "20         0.002876           0.000000  \n",
       "19         0.007531           0.000000  \n",
       "18         0.003117           0.000000  \n",
       "17         0.008430           0.000000  \n",
       "\n",
       "[7665 rows x 9 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=[\"above_73\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3561f",
   "metadata": {},
   "source": [
    "Sad, it got worse. The raw feature removing journey might end here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c54044c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26890769 0.1834    ]\n",
      " [0.11764615 0.43004615]]\n"
     ]
    }
   ],
   "source": [
    "#889\n",
    "n_split = 5 \n",
    "n_repeats = 20\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=n_split, random_state=420, n_repeats=n_repeats)\n",
    "splits = list(RSKF.split(X=feat4, y=tar))\n",
    "\n",
    "pipe=Pipeline([\n",
    "    (\"DataCreate\", training.data_creator()),\n",
    "    (\"DataSelector\", training.data_selector(force=[\"X1\",\"X6\",\"F_w_mean\"])),\n",
    "    (\"scale\",StandardScaler()),\n",
    "    (\"KNN\",KNeighborsClassifier(n_neighbors=5))]\n",
    ")\n",
    "\n",
    "training.show_result(splits=splits, pipe=pipe, feat=feat4, tar=tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca8a4122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.27733846 0.17496923]\n",
      " [0.12987692 0.41781538]]\n"
     ]
    }
   ],
   "source": [
    "#919\n",
    "n_split = 5 \n",
    "n_repeats = 20\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=n_split, random_state=420, n_repeats=n_repeats)\n",
    "splits = list(RSKF.split(X=feat4, y=tar))\n",
    "\n",
    "pipe=Pipeline([\n",
    "    (\"DataCreate\", training.data_creator()),\n",
    "    (\"DataSelector\", training.data_selector(force=[\"X1\",\"X6\",\"above_4\"])),\n",
    "    (\"scale\",StandardScaler()),\n",
    "    (\"KNN\",KNeighborsClassifier(n_neighbors=5))]\n",
    ")\n",
    "\n",
    "training.show_result(splits=splits, pipe=pipe, feat=feat4, tar=tar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_3_11_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
